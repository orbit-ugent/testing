{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESS: Introduction to geophysical data analysis with Python\n",
    "---\n",
    "This introductory notebook will help you explore frequency domain electromagnetic induction (FDEM) data, and help get you acquainted with basic data operations in Python. It contains code that you can readily use in the 2023 Environmental Soil Sensing practicum.<br> \n",
    "\n",
    "This [jupyter notebook](https://docs.jupyter.org/en/latest/) is made up of text cells (markdown) like this one, and code cells (with [Python](https://docs.python.org/3/tutorial/) code), such as the two cells below these. Note that in code cells, there also is text (comments), which is preceded by a shebang (`# This is a comment`) or placed between two sets of three parentheses (`'''This is a comment'''`).\n",
    "\n",
    "In these first cells you will install and import specific packages (ready-made sets of code) that you will use to perform analyses in this notebook. \n",
    "\n",
    "If you are unfamiliar with Python code and jupyter notebooks, you find some introductory notebooks in the [DS-python-geospatial repository](https://github.com/jorisvandenbossche/DS-python-geospatial/tree/main/notebooks), developed by Joris Van den Bossche. However, since you are using this notebook with Google Colaboratory, all steps are automated and you should have a smooth experience in using all this! Extensive documentation is provided in these code cells, to help you along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages in Google Colaboratory (do not run this cell when \n",
    "# not working in Google Colaboratory).\n",
    "'''\n",
    "installs the following packages:\n",
    "  - clhs: package to perform conditioned latin hypercube sampling (Minasny & McBratney, 2005)\n",
    "'''\n",
    "!pip install clhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "'''\n",
    "Import the required modules to run all code in this notebook.\n",
    "'''\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.core.display import display\n",
    "from ipywidgets import widgets, HBox\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import cKDTree\n",
    "import clhs\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data exploration\n",
    "***\n",
    "First, you will explore the data you are working with. We do this by using the [pandas package](https://pandas.pydata.org/) in Python, which allows you to manipulate and analyse data in a straigthforward way. As you will see, the data you load (.csv-file) are structured in rows and columns that hold the FDEM measurement data. After loading the different datasets you will use in this assignment, you can start evaluating the different datasets that have been collected.\n",
    "\n",
    "The table below lists all columns of the FDEM datasets, and explains their datatype.\n",
    "\n",
    "> \n",
    ">|Column name|datatype|\n",
    ">|-----------|--------|\n",
    ">| *x* | easting [m]|\n",
    ">| *y* | northing [m]|\n",
    ">| *z* | elevation [m]|\n",
    ">| *t* | timestamp [s]|\n",
    ">| *HCP0.5* | 0.5 m HCP LIN ECa data [mS/m]|\n",
    ">| *PRP0.6* | 0.6 m PRP LIN ECa data [mS/m]|\n",
    ">| *HCP1.0* | 1.0 m HCP LIN ECa data [mS/m]|\n",
    ">| *PRP1.1* | 1.1 m PRP LIN ECa data [mS/m]|\n",
    ">| *HCP2.0* | 2.0 m HCP LIN ECa data [mS/m]|\n",
    ">| *PRP2.1* | 2.1 m PRP LIN ECa data [mS/m]|\n",
    ">| *HCP0.5_inph* | 0.5 m HCP inphase data [ppt]|\n",
    ">| *PRP0.6_inph* | 0.6 m PRP inphase data [ppt]|\n",
    ">| *HCP1.0_inph* | 1.0 m HCP inphase data [ppt]|\n",
    ">| *PRP1.1_inph* | 1.1 m PRP inphase data [ppt]|\n",
    ">| *HCP2.0_inph* | 2.0 m HCP inphase data [ppt]|\n",
    ">| *PRP2.1_inph* | 2.1 m PRP inphase data [ppt]|\n",
    ">\n",
    ">*The x and y coordinates are presented in meters Belge Lambert 72 (EPSG:31370), and the elevation (z) in meters above sea level (z).* \n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset location\n",
    "'''\n",
    "Here you create variables that hold the location of the different datasets you\n",
    "will use as strings.\n",
    "\n",
    "FDEM_surveydata: full example survey dataset of the Bottelare testfield\n",
    "FDEM_transect: extract of the survey dataset along a reference transect. You can\n",
    "                use this to test analytical procedures, which you can then \n",
    "                deploy across the full survey dataset\n",
    "cLHS_calibration: calibration sample dataset\n",
    "cLHS_validation: validation sample dataset\n",
    "'''\n",
    "\n",
    "FDEM_surveydata = 'https://users.ugent.be/~pjdsmedt/ESS2023/FDEM_bott.csv'\n",
    "FDEM_transect = 'https://users.ugent.be/~pjdsmedt/ESS2023/FDEM_transect.csv'\n",
    "cLHS_calibration = 'https://users.ugent.be/~pjdsmedt/ESS2023/cLHS_calibration.csv'\n",
    "cLHS_validation = 'https://users.ugent.be/~pjdsmedt/ESS2023/cLHS_validation.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas dataframes\n",
    "-----------------\n",
    "Data are often presented in spreadsheets, ascii files, or dataframes. In Python, dataframes are defined as two-dimensional data structures (i.e., data arranged in rows and columns) and can be manipulated with the [Pandas](https://pandas.pydata.org/) package.\n",
    "The FDEM data are available as a comma-separated filed (.csv-file), which you can load into a pandas dataframe in the next cell. \n",
    "After loading the data, you can explore the different dataframes directly, either by going through the rows and columns or by looking at its statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset - any time when you want to reset your processing, run this cell.\n",
    "'''\n",
    "The loaded datasets are .csv-files. These are read and structured with the \n",
    "pandas (imported above as pd) package. In this code cell, you load all datasets\n",
    "into pandas dataframes. If you leave the lines below unchanged, these are:\n",
    "\n",
    "    - df = dataframe with the full FDEM dataset\n",
    "    - dt = dataframe with the FDEM transect\n",
    "    - ds = datasframe with the sample data \n",
    "'''\n",
    "df = pd.read_csv(FDEM_surveydata, sep=',', header=0)\n",
    "dt = pd.read_csv(FDEM_transect, sep=',', header=0)\n",
    "d_cal = pd.read_csv(cLHS_calibration, sep=',', header=0)\n",
    "d_val = pd.read_csv(cLHS_validation, sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore first 5 rows of the dataframe\n",
    "'''\n",
    "You can perform a wide range of operations on pandas dataframes. These are \n",
    "performed by calling 'methods', by simply typing these after the dataframe's \n",
    "identifier.\n",
    "\n",
    "In this cell, you call the 'head' method, which returns the first n rows of the\n",
    "dataframe. Methods can take in arguments depending on their functionality, which\n",
    "are typed between brackets. The head method takes a number of n rows as \n",
    "argument. \n",
    "\n",
    "If you call the head method without arguments (like: df.head()), the number of\n",
    "rows you get to see defaults to n = 5. This gives the same result as df.head(5).\n",
    "If you want to visualise the first 10 rows, you pass 10 as the argument, like \n",
    "this: df.head(10).\n",
    "'''\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore statistics of the FDEM measurement data\n",
    "''' \n",
    "Here you perform a different set of operations on the dataframe. By using the\n",
    "'iloc' method, you can select a specific range of columns and/or rows of a\n",
    "dataframe. \n",
    "In the example below, we evaluate all columns except the first five.\n",
    "We do this based on the indices of the columns. \n",
    "In the example, we evaluate all rows (:) for column five (,5) and all columns \n",
    "from column five onwards (:). So: [:,5:] \n",
    "\n",
    "If you would want to evaluate rows 5 up to 10, of columns 11 up to 16 you would \n",
    "write:\n",
    "    df.iloc[5:10,10:16]\n",
    "You can check this by uncommenting (removing the #) from the first line \n",
    "below this comment block, and commenting the second line of code (adding a #).\n",
    "\n",
    "The second operation that is performed here, is the describe() method. This \n",
    "provides the descriptive statistics of the dataframe. Adding the iloc method \n",
    "allows you to select a specific part of the dataframe. Removing the iloc method \n",
    "as 'df.describe())' would output the statistics of the entire dataframe.\n",
    "'''\n",
    "\n",
    "df.iloc[5:10,10:16].head()\n",
    "df.iloc[:,5:].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualization\n",
    "------------------\n",
    "Here you can plot the data based on its geographical coordinates. For this, you use the module 'pyplot' from the package 'matplotlib' (imported as 'plt').\n",
    "In the code block below, you have examples of how to plot point datasets as a scatterplot (\"plt.scatter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GPS coordinates (x,y,z) as a scatterplot. \n",
    "'''\n",
    "General plotting example, plotting the values of the dataframe columns\n",
    "containing the x & y coordinates, and the elevation as a scatterplot. \n",
    "For this, you use the Python package Matplotlib (a submodule imported as 'plt').\n",
    "\n",
    "In this example, you plot the FDEM survey data in the dataframe 'df', and\n",
    "overlay it with the reference transect in dataframe 'dt'. \n",
    "You make explicit that you want to work with the values of the dataframe \n",
    "by calling the '.values' method of the dataframe. You can call specific \n",
    "columns by specifying their header as df['headername'].\n",
    " \n",
    "'''\n",
    "plt.figure(figsize=(10, 10))\n",
    "# plotting the 'df' dataframe as a scatterplot\n",
    "plt.scatter(df['x'].values,     # x values of the dataframe\n",
    "            df['y'].values,     # y values of the dataframe\n",
    "            c=df['z'].values,   # z values, used the colorgrade the scatterplot\n",
    "                                # markers\n",
    "            s=.5,               # size of the scatterplot markers\n",
    "            cmap = 'copper')    # colormap for the scatterplot\n",
    "\n",
    "# plotting the 'dt' dataframe as a line\n",
    "plt.plot(dt['x'].values,\n",
    "        dt['y'].values,\n",
    "        color = 'blue',\n",
    "        label = 'FDEM transect')\n",
    "plt.colorbar().set_label('Elevation (m)')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Lambert72 E (m)')\n",
    "plt.ylabel('Lambert72 N (m)')\n",
    "plt.yticks(rotation = 90)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting measurement data\n",
    "In the cell below, you are again plotting the FDEM survey data, only this time, you will plot the output of the data recorded with the various coil configurations. \n",
    "By changing the coil geometry type (HCP0.5 - 2.0 or PRP0.6 - 2.1), and the signal component (inphase = False/True), you can visualise the survey measurement data. This will  show you the spatial variation in the data as a first basis for interpretation.<br><br>\n",
    "You can specify if you want to plot the inphase data or the ECa data by setting the boolean inphase variable to True or False, respectively. \n",
    "In the second cell below this text, you can plot the histograms for the IP and QP responses for a given coil geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coil configuration\n",
    "cgeometry = 'HCP1.0' # specify coil geometry ('HCP0.5', 'HCP1.0', 'HCP2.0', \n",
    "                     # 'PRP0.6', 'PRP1.1', 'PRP2.1')\n",
    "inphase = False # boolean, False = ECa data; True = inphase data (in ppt)\n",
    "\n",
    "if inphase:\n",
    "    cc = cgeometry + '_inph'\n",
    "    label = 'IP [ppt]'\n",
    "    colorscale = 'Greys'\n",
    "else:\n",
    "    cc = cgeometry\n",
    "    label = 'ECa [mS/m]'\n",
    "    colorscale = 'viridis_r'\n",
    "\n",
    "# Colorbar range\n",
    "cmin_percentile = 2  # %\n",
    "cmax_percentile = 98  # %\n",
    "lim0 = np.percentile(df[cc].values, cmin_percentile)\n",
    "lim1 = np.percentile(df[cc].values, cmax_percentile)\n",
    "\n",
    "# Visualize the survey itself\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.scatter(df['x'].values, \n",
    "            df['y'].values, \n",
    "            c=df[cc].values,\n",
    "            s=1., \n",
    "            cmap = colorscale)\n",
    "\n",
    "plt.colorbar().set_label(label)\n",
    "plt.clim(lim0, lim1)\n",
    "plt.gca().set_ylabel(label)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Lambert72 E (m)')\n",
    "plt.ylabel('Lambert72 N (m)')\n",
    "plt.yticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize the histogram of the IP and ECa data \n",
    "# from the coil geometry specified in the cell above.\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "cci = cgeometry + '_inph'\n",
    "plt.hist(df[cci].values, bins=100, log=True)\n",
    "plt.vlines([np.percentile(df[cci].values, cmin_percentile), \n",
    "            np.percentile(df[cci].values, cmax_percentile)], \n",
    "           0, 1e5, \n",
    "           colors='red')\n",
    "plt.xlabel(cgeometry + ' IP [ppt]')\n",
    "plt.ylabel('Counts (-)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "ccq = cgeometry\n",
    "plt.hist(df[ccq].values, bins=100, log=True)\n",
    "plt.vlines([np.percentile(df[ccq].values, cmin_percentile), \n",
    "            np.percentile(df[ccq].values, cmax_percentile)], \n",
    "           0, 1e5, \n",
    "           colors='red')\n",
    "plt.xlabel(cgeometry +' ECa [mS/m]')\n",
    "plt.ylabel('Counts (-)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Spatial interpolation\n",
    "---\n",
    "\n",
    "If we want to move the data space from scattered point observations towards evenly spread observations or raster data we need to make use of a spatial interpolation algorithm.\n",
    "Examples of these algorithms include nearest neighbor interpolation, linear and cubic interpolation.\n",
    "\n",
    "You can perform such a simple interpolation in the cells below. For this, you have to define an evenly spaced grid to which the algorithm has to interpolate to.\n",
    "The interpolation takes the [convex hull](https://en.wikipedia.org/wiki/Convex_hull) as standard boundaries for the interpolation, so there unwanted extrapolation is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here you perform a simple linear interpolation (alternatively, nearest \n",
    "neighbor or cubic interpolation can be performed as well by selecting the \n",
    "desired method in the 'griddata' function). \n",
    "For interpolating, you simply have to specify the dataset you want to \n",
    "interpolate (dataset), and the cell size of the final raster (\n",
    "i.e., the resolution of the output grid).\n",
    "'''\n",
    "\n",
    "# Input variables\n",
    "dataset = 'HCP1.0' # column of data values to interpolate\n",
    "cell_size = 0.5  # raster cell size in meters\n",
    "\n",
    "# Define an evenly spaced grid over which the dataset values have to be \n",
    "# interpolated\n",
    "x_min = df['x'].min()\n",
    "x_max = df['x'].max()\n",
    "y_min = df['y'].min()\n",
    "y_max = df['y'].max()\n",
    "x_vector = np.arange(x_min, x_max, cell_size)\n",
    "y_vector = np.arange(y_min, y_max, cell_size)\n",
    "\n",
    "# Interpolate \n",
    "xx, yy = np.meshgrid(x_vector, y_vector)\n",
    "data_grid = griddata(np.vstack((df['x'].values, df['y'].values)).T, \n",
    "                df[dataset].values, \n",
    "                (xx, yy), \n",
    "                method='linear')  # other methods (nearest, cubic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "With this code, you can plot your interpolated dataset.\n",
    "'''\n",
    "\n",
    "# Check if inphase or ECa data and set parameters accordingly\n",
    "if 'inph' not in dataset:\n",
    "        colorscale = 'viridis_r'\n",
    "        label = 'ECa [mS/m]'\n",
    "else:        \n",
    "        colorscale = 'Greys'\n",
    "        label = 'IP [ppt]'\n",
    "\n",
    "# Plot interpolated data\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(\n",
    "        data_grid, \n",
    "        origin='lower', \n",
    "        extent=[x_min, x_max, y_min, y_max],\n",
    "        cmap = colorscale\n",
    "        )\n",
    "\n",
    "# set plot range based on dataset value percentiles\n",
    "plt.clim(\n",
    "        np.percentile(df[dataset].values, 2), \n",
    "        np.percentile(df[dataset].values, 98)\n",
    "        )\n",
    "plt.colorbar().set_label(label)\n",
    "plt.axis('equal')\n",
    "plt.title(dataset)\n",
    "plt.xlabel('LAM72 X (m)')\n",
    "plt.ylabel('LAM72 Y (m)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Environmental sampling with ancillary information\n",
    "---\n",
    "## Conditioned Latin Hypercube Sampling\n",
    "\n",
    "[Conditioned Latin Hypercube Sampling](https://doi.org/10.1016/j.cageo.2005.12.009) is a form of latin hypercube sampling that assures selected samples are maximally stratified across the multivariate dataspace, and assures that each sampling point represents a real combination of the multivariate variables. It is an extension of regular latin hypercube sampling, whereby samples are taken across the multivariate distribution. \n",
    "The conditioning inherent to cLHS is based on ancillary data, and implies that the hypercube is formed based on the feature space (real multivariate data values). This results in a true or approximate Latin hypercube of the feature space whereby the distribution and multivariate correlation will be preserved.\n",
    "\n",
    "Details on both traditional LHS and cLHS can be found in [Minasny & McBratney (2005)](https://doi.org/10.1016/j.cageo.2005.12.009).\n",
    "\n",
    "From the above, the reason for applying cLHS to design a sampling scheme for our study area should be apparent: we have at our disposal an exhaustive, multivariate spatial dataset, which serves as an ideal basis for designing a stratified random sampling approach.\n",
    "\n",
    "Below you can test different combinations of FDEM datasets for the study area, and design your own sampling scheme using the cLHS algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set sample number\n",
    "number_of_samples = 10\n",
    "\n",
    "# determine the input variables (x, y-coordinates, and then the measurement data\n",
    "# you want to include. In the example below this is the PRP1.1 and the HCP2.0 \n",
    "# ECa data).\n",
    "sigin = ['x', 'y','PRP1.1', 'HCP2.0'] #\n",
    "\n",
    "# cLHS\n",
    "sampled = clhs.clhs(df[sigin], number_of_samples, max_iterations=10000)\n",
    "clhs_sample = df.iloc[sampled['sample_indices']]\n",
    "\n",
    "'''\n",
    "Try to plot the resulting sample dataset on top of the interpolated map you\n",
    "have created. You can base this on the plotting codes provided in the cells\n",
    "above. You will see that on each run of the cLHS algorithm, a different result\n",
    "is generated.\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available sample data\n",
    "\n",
    "In preparation for this exercise, latin hypercube sampling was already performed using an available FDEM dataset. \n",
    "Samples were taken by considering the x, y coordinates, and the ECa data from the PRP 1.1 and HCP 2.0 m coil configurations. <br>\n",
    "Two sampling datasets were collected. A calibration dataset (`d_cal`), consisting of 10 samples, and a validation dataset (`d_val`), consisting of 5 samples. In both cases, the cLHS algorithm was run separately to generate the sample datasets.\n",
    "At each sampling location, soil samples were taken at two depth intervals below the surface: at 10-15 cm, and at 50-55 cm depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here you can visualise the sampling locations of the available cLHS dataset\n",
    "plotted over the dataset you have interpolated above.\n",
    "'''\n",
    "\n",
    "# visualing the interpolated dataset\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(data_grid, origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "plt.colorbar().set_label('ECa (mS/m)')\n",
    "plt.clim(np.percentile(df[dataset].values, 2), np.percentile(df[dataset].values, 98))\n",
    "\n",
    "# plotting the 'd_cal' dataframe as points (with their sampling ID)\n",
    "marker_color = 'red'\n",
    "font_color = 'white'\n",
    "plt.scatter(d_cal['x'].values,\n",
    "        d_cal['y'].values,\n",
    "        color = marker_color,\n",
    "        label = 'cLHS - calibration')\n",
    "for (xi, yi, zi) in zip(d_cal['x'].values, d_cal['y'].values, d_cal['id'].values):\n",
    "    plt.text(xi, yi, zi, va='center_baseline', \n",
    "             ha='center', \n",
    "             color = font_color, \n",
    "             backgroundcolor = marker_color)\n",
    "\n",
    "# plotting the 'd_val' dataframe as points (with their sampling ID)\n",
    "marker_color = 'green'\n",
    "font_color = 'white'\n",
    "plt.scatter(d_val['x'].values,\n",
    "        d_val['y'].values,\n",
    "        color = marker_color,\n",
    "        label = 'cLHS - validation')\n",
    "for (xi, yi, zi) in zip(d_val['x'].values, d_val['y'].values, d_val['id'].values):\n",
    "    plt.text(xi, yi, zi, \n",
    "             va='center_baseline', \n",
    "             ha='center', \n",
    "             color = font_color,  \n",
    "             backgroundcolor = marker_color)\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "\n",
    "If you want to extract the data values at the sampling locations, there are two ways to do this. \n",
    "First, you can simply query the dataframe that you want to extract the values from (e.g., `df`), to extract the data at the sampled locations (i.e., the x,y-coordinates in `d_cal` or `d_val`). Since samples are taken at coordinates that occur in the survey dataset (`df`), the exact same coordinates are present in the sampling (`d_cal` and `d_val`) and survey (`df`) dataframes.\n",
    "You can extract the coordinates by performing an inner merge operation as in the first cell below.\n",
    "\n",
    "Alternatively, you can perform a k-nearest neighbor search using [scipy]('https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.query.html'). This is useful if you are unsure if the coordinates are exactly the same in both datasets. The second code cell perfoms this operation for reference.\n",
    "\n",
    "These operations are useful to compare different datasets or datatypes, and if you want to perform regressions and predictions based on these data. <br> Below the first two code cells, an example is given on how to perform a [linear regression with Scikit learn](https://scikit-learn.org/stable/modules/linear_model.html), and [polynomial regression using Numpy](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pandas method: inner merge of dataframes\n",
    "----------------------------------------\n",
    "Merge two dataframes and only keep part of the dataframe in which the values \n",
    "of specified columns are the same. In this case, you only keep part of the \n",
    "dataframe where the x-y coordinates in both evaluated dataframes are exactly \n",
    "the same.\n",
    "'''\n",
    "\n",
    "calibration_set = pd.merge(df, d_cal, how='inner', on=['x', 'y'])\n",
    "validation_set = pd.merge(df, d_val, how='inner', on=['x', 'y'])\n",
    "\n",
    "# plotting the outcomes\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# visualing the interpolated dataset\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(data_grid, origin='lower', extent=[x_min, x_max, y_min, y_max])\n",
    "plt.colorbar().set_label('ECa (mS/m)')\n",
    "plt.clim(np.percentile(df[dataset].values, 2), np.percentile(df[dataset].values, 98))\n",
    "\n",
    "plt.scatter(d_cal['x'].values,\n",
    "        d_cal['y'].values,\n",
    "        color = 'black', # marker color\n",
    "        s = 50, # marker size\n",
    "        label = 'cLHS - calibration')\n",
    "plt.scatter(calibration_set['x'].values,\n",
    "        calibration_set['y'].values,\n",
    "        color = 'yellow',\n",
    "        s = 10,\n",
    "        label = 'cLHS - calibration')\n",
    "\n",
    "plt.scatter(d_val['x'].values,\n",
    "        d_val['y'].values,\n",
    "        color = 'blue',\n",
    "        s = 50,\n",
    "        label = 'cLHS - validation')\n",
    "plt.scatter(validation_set['x'].values,\n",
    "        validation_set['y'].values,\n",
    "        color = 'white',\n",
    "        edgecolors = 'black',\n",
    "        s = 15,\n",
    "        label = 'cLHS - validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scipy method - knn search\n",
    "-------------------------\n",
    "Perform a knn-search on two dataframes to identify points that are closest\n",
    "between columns of an input dataframe (in the case below 'df'), and a query\n",
    "dataframe (in the case below the calibration sample dataset 'd_cal').\n",
    "'''\n",
    "\n",
    "# transform dataframes to numpy arrays and perform knn search\n",
    "data_in = np.array(list(zip(df['x'].values,df['y'].values)) )\n",
    "data_query = np.array(list(zip(d_cal['x'].values,d_cal['y'].values)) )\n",
    "btree = cKDTree(data_in)\n",
    "dist, idx = btree.query(data_query, k=1) # k = number of neighbors; \n",
    "                                         # idx = index of the neighbors\n",
    "                                         # dist = distance between the neighbors       \n",
    "\n",
    "# Concatenate the survey data values at the location closest to the \n",
    "# queried dataset based on the indices 'idx' (df.iloc[idx]), and the queried \n",
    "# data themselves.\n",
    "calibration_set = pd.concat(\n",
    "    [\n",
    "    df.iloc[idx],   # part of original dataframe that is closest to queried data                    \n",
    "    d_cal.set_index(df.iloc[idx].index) # reset index of the queried dataframe\n",
    "    .rename(columns = {'x':'x_clhs','y':'y_clhs'}) # and rename the x-y columns\n",
    "    ],\n",
    "    axis=1 #concatenate along the correct axis (i.e., along the columns)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here it is illustrated how you can perform a linear or polynomial regression\n",
    "on evaluated datasets. You can create suited models to predict a target \n",
    "variable. In the example below, the scikit-learn (sklearn) is used to perform\n",
    "a linear regression, while numpy (np) is used to do a polynomial regression.\n",
    "\n",
    "In this example, a regression is made between two of the EMI datasets, which has\n",
    "no practical sense. In reality, you can use these functions to investigate the \n",
    "relationship between sensor data and target properties at sample locations, to\n",
    "drive stochastic modelling procedures.\n",
    "'''\n",
    "# specify which dataframe you want to use (calibration_set, validation_set, df).\n",
    "#       you can also combine the calibration and validation sets like this:\n",
    "#       >> dataframe = pd.concat([calibration_set, validation_set])\n",
    "\n",
    "dataframe = df\n",
    "\n",
    "# specify the columns of the dataframe you want to use.\n",
    "column_1 = 'PRP1.1'\n",
    "column_2 = 'HCP2.0'\n",
    "\n",
    "# assign these to two variables, dataset 1 and 2\n",
    "dataset_1 = dataframe[column_1].values\n",
    "dataset_2 = dataframe[column_2].values\n",
    "\n",
    "'''\n",
    "to directly use a column of a pandas dataframe in sklearn, we have to make \n",
    "explicit the dimensions of the array. You can do this by converting the \n",
    "dataframe or dataframe column to a numpy array, or simply by reshaping the \n",
    "column you're working with to the correct form as done below.\n",
    "'''\n",
    "\n",
    "len_1 = len(dataset_1)\n",
    "array_1 = dataset_1.reshape(len_1,1)\n",
    "array_2 = dataset_2.reshape(len_1,1)\n",
    "\n",
    "# Perform a linear regression with Scikit-learn (sklearn)\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "lin_reg.fit(array_1,array_2) # perform a fit on both datasets\n",
    "lin_pred = lin_reg.predict(array_1)\n",
    "\n",
    "# Perform a polynomial regression with Numpy (np)\n",
    "poly_degree = 2\n",
    "poly_fit = np.poly1d(np.polyfit(dataset_1, dataset_2, poly_degree))\n",
    "poly_range = np.linspace(\n",
    "                         np.floor(dataset_1.min()),\n",
    "                         np.ceil(dataset_1.max()),\n",
    "                         100\n",
    "                         )\n",
    "poly_pred = poly_fit(np.sort(dataset_1)) # get the polynomial regression \n",
    "                                         # across the desired range\n",
    "\n",
    "# Get the coefficient of determination of the performed regressions\n",
    "#   for this we simply use the sklearn function r2_score\n",
    "\n",
    "lin_score = r2_score(dataset_2,lin_pred)\n",
    "ply_score = r2_score(np.sort(dataset_2),poly_pred)\n",
    "\n",
    "# Plot the results\n",
    "#   the rounded r-squared values are given in the legend (convert float to \n",
    "#   string using >str(); rounding to 3 digits using >np.round()).\n",
    "\n",
    "plt.scatter(dataset_1, dataset_2,  color='gray')\n",
    "plt.plot(dataset_1,\n",
    "            lin_pred, \n",
    "            color='blue', \n",
    "            linewidth=3, \n",
    "            label = 'Linear, R2 = ' + str(np.round(lin_score,3))\n",
    "            )\n",
    "plt.plot(np.sort(dataset_1), \n",
    "            poly_pred, \n",
    "            color = 'red', \n",
    "            label = 'Polynomial, R2 = ' + str(np.round(ply_score,3))\n",
    "            )\n",
    "plt.xlabel(column_1)\n",
    "plt.ylabel(column_2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "64741f101c4c61109c751d1cf312fb9d03f24ebf08ef89d3abcfdde998172cbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
