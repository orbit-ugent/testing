{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESS: Analysing and interpreting FDEM data\n",
    "---\n",
    "# 0 - Introduction\n",
    "In this notebook you will explore FDEM data in a more in depth way, and relate the outcomes to specific soil properties. Hereby, you will combine geophysical modelling (inversion) with pedophysical modelling, alongside relating specific target properties to the FDEM outcomes in a more stochastic way. \n",
    "\n",
    "This notebook consists of Markdown (text) cells, such as this one, and code cells like the one below. Each code cell is numbered for reference. The first code cell below is **_code cell 0.0_**. Running that cell and **_code cell 0.1_**  installs the required packages in Google Colaboratory, and subsequently import all required packages into the workspace.\n",
    "\n",
    "There is a lot of Python code in this notebook. However, while you are free to modify the code as you want, in cases where you have to perform data analyses, the part of the code where you have to modify variables, or write functions, always appears above a commented line of asterisks, like this: `# ******* `. The part below the asterisk line is where the rest of the code is written that is required to perform the operation, but where to fulfill the practicum you are not required to change anything.\n",
    "\n",
    "If no asterisk line is present in a code cell, this means you can simply run the code cell without changing anything to get the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.0: Install required packages in Google Colaboratory\n",
    "# -----------------------------------------------------\n",
    "'''\n",
    "installs the following packages:\n",
    "  - emagpy: package to analyse and invert FDEM data\n",
    "  - rasterio: package to handle and manipulate geospatial raster data\n",
    "'''\n",
    "!pip install emagpy\n",
    "!pip install geopandas\n",
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1: Import packages into workspace\n",
    "# -----------------------------------------------------\n",
    "'''\n",
    "Import the required modules to run all code in this notebook.\n",
    "'''\n",
    "# General utility modules\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Modules for geopunt data visualisation\n",
    "from IPython.display import HTML\n",
    "from IPython.core.display import display\n",
    "from ipywidgets import widgets, HBox\n",
    "\n",
    "# Data visualisation, manipulation, and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.spatial import cKDTree\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.optimize import root\n",
    "\n",
    "# Geospatial data manipulation and raster operations\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Electromagnetic induction data inversion package\n",
    "from emagpy import Problem\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field dataset - Testfield Proefhoeve Bottelaere [Vijverhoek, Oosterzele]\n",
    "\n",
    "The key dataset you will work with in this notebook, is the frequency-domain electromagnetic induction (FDEM) dataset that was collected at our testsite on April 28, 2023. Data were collected with a Dualem-21S. Based on the soil map, soil texture in the test site ranges from sandy loam to clay. WRB soil groups are Cambisols (Eutric Stagnic Cambisols (Loamic)) and Stagnosols (Dystric Retic Stagnosols (Loamic)), both are generally Ruptic (referring to the occurrence of two different source materials, in this case aeolian sand/loess over Tertiary clay). You can explore the soil map, along with the WRB classification, in **_code cell 0.2_**.\n",
    "\n",
    "The data you have are collected with a FDEM instrument with the following specifications:\n",
    "- operating frequency: 9000 Hz\n",
    "- coil geometries:\n",
    "    1. three coil pairs in HCP mode with Rx at 0.5 m (HCP0.5), 1.0 m (HCP2.1), 2.0 m (HCP2.0) from Tx.\n",
    "    2. three coil pairs in PRP mode with Rx at 0.6 m (PRP0.6), 1.1 m (PRP1.1), 2.1 m (PRP2.1) from Tx.\n",
    "- output: QP data as LIN ECa [mS/m], IP data as field intensity [ppt].\n",
    "\n",
    "Table 1 below lists all columns of the FDEM datasets, and explains their datatype.\n",
    "\n",
    "> \n",
    ">|Column name|datatype|\n",
    ">|-----------|--------|\n",
    ">| *x* | easting [m]|\n",
    ">| *y* | northing [m]|\n",
    ">| *z* | elevation [m]|\n",
    ">| *t* | timestamp [s]|\n",
    ">| *HCP0.5* | 0.5 m HCP LIN ECa data [mS/m]|\n",
    ">| *PRP0.6* | 0.6 m PRP LIN ECa data [mS/m]|\n",
    ">| *HCP1.0* | 1.0 m HCP LIN ECa data [mS/m]|\n",
    ">| *PRP1.1* | 1.1 m PRP LIN ECa data [mS/m]|\n",
    ">| *HCP2.0* | 2.0 m HCP LIN ECa data [mS/m]|\n",
    ">| *PRP2.1* | 2.1 m PRP LIN ECa data [mS/m]|\n",
    ">| *HCP0.5_inph* | 0.5 m HCP inphase data [ppt]|\n",
    ">| *PRP0.6_inph* | 0.6 m PRP inphase data [ppt]|\n",
    ">| *HCP1.0_inph* | 1.0 m HCP inphase data [ppt]|\n",
    ">| *PRP1.1_inph* | 1.1 m PRP inphase data [ppt]|\n",
    ">| *HCP2.0_inph* | 2.0 m HCP inphase data [ppt]|\n",
    ">| *PRP2.1_inph* | 2.1 m PRP inphase data [ppt]|\n",
    ">\n",
    "> *Table 1: overview of FDEM data column names and the datatype these hold.*\n",
    ">*(The x and y coordinates are presented in meters Belge Lambert 72 (EPSG:31370), and the elevation (z) in meters above sea level.)* \n",
    "\n",
    "Basic processing has been performed, which entails:\n",
    "- accurate georeferencing and projection to Belge Lambert 1972 coordinate system,\n",
    "- removal of erroneous datapoints (e.g., standstill moments when the survey setup stopped for >5 seconds),\n",
    "- drift correction (of all IP and QP/ECa datasets).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2: Exploring test site geodata from 'geopunt.be'\n",
    "# --------------------------------------------------\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.core.display import display\n",
    "from ipywidgets import widgets, HBox\n",
    "\n",
    "airph = '<iframe src=\"https://www.geopunt.be/embed/d1fa24f5-a258-45b1-887d-8c1f6928a437\" width=\"500\" height=\"600\"></iframe>'\n",
    "soilm = '<iframe src=\"https://www.geopunt.be/embed/e7d14f05-9d72-4df1-b8ff-4a2b2a9311db\" width=\"500\" height=\"600\"></iframe>'\n",
    "\n",
    "AerialPhoto = widgets.HTML(airph)\n",
    "SoilMap = widgets.HTML(soilm)\n",
    "\n",
    "twinbox = HBox([AerialPhoto, SoilMap])\n",
    "display(twinbox)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "In **_code cell 0.3_**, you can load all survey data that was collected during the practicum. \n",
    "These include:\n",
    "- FDEM_surveydata: full survey dataset of the Bottelare testfield;\n",
    "- FDEM_transect: extract of the survey dataset along a reference transect. You can use this to test analytical procedures, which you can then deploy across the full survey dataset;\n",
    "- sampling_data: calibration and validation sample dataset, collected as two independent stratified random sample sets (samples 1 - 10 =  calibration; 11 - 15 =  validation).\n",
    "\n",
    "The sampling_data dataset includes the x and y coordinates of the samples along with their ID's, and analytical data on:\n",
    "\n",
    "- bulk density in g/cm^3   (`bd [g/cm3]`);\n",
    "- volumetric water content in % (`vwc [%]`);\n",
    "- clay content in %   (`clay [%]`);\n",
    "- soil organic matter in %   (`SOM [%]`);\n",
    "- gravimetric water content (`gmc [%]`);\n",
    "- silt content in %   (`silt [%]`);\n",
    "- sand content in %   (`sand [%]`);\n",
    "- porosity [dimensionless]   (`por [-]`);\n",
    "- total organic carbon in g/kg   (`SOC [g/kg]`);\n",
    "\n",
    "- hydraprobe volumetric water content in %   (`hydra_vwc [%]`)\n",
    "- hydraprobe raw EC in mS/m   (`hydra_ec [mS/m]`)\n",
    "- hydraprobe soil pore water EC in mS/m   (`hydra_ecp [mS/m]`)\n",
    "- hydraprobe soil temperature in C   (`hydra_tmp [C]`)\n",
    "- hydraprobe temperature corrected EC in mS/m   (`hydra_ect [mS/m]`)\n",
    "- hydraprobe dielectric permittivity [dimensionless]   (`hydra_er [-]`).\n",
    "\n",
    "The column names are shown between the brackets above. \n",
    "You can use these to access the desired data values in the `ds` dataframe (e.g., for bulk density, the values are in the `ds['bd [g/cm3]]` column).\n",
    "\n",
    "**All geospatial datasets are provided in Lambert 1972 coordinates** (EPSG:31370).\n",
    "At any point, if the dataframe structure is unclear, or you want to see the column names, you can use the built-in functions df.head() or df.columns. You can also simply download the .csv files and open these in excel or your preferred software.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.3: Get dataset location\n",
    "# -------------------------\n",
    "'''\n",
    "Create variables with dataset URLs\n",
    "----------------------------------\n",
    "FDEM_surveydata: full survey dataset of the Bottelare testfield\n",
    "FDEM_transect: extract of the survey dataset along a reference transect. You can\n",
    "                use this to test analytical procedures, which you can then \n",
    "                deploy across the full survey dataset\n",
    "sampling_data: calibration and validation sample dataset (samples 1 - 15)\n",
    "'''\n",
    "# store dataset URL's as string variables\n",
    "FDEM_surveydata = 'https://users.ugent.be/~pjdsmedt/ESS2023/FDEM_2804.csv'\n",
    "FDEM_transect = 'https://users.ugent.be/~pjdsmedt/ESS2023/FDEM_transect_2804.csv'\n",
    "samples = 'https://users.ugent.be/~pjdsmedt/ESS2023/samples_combined.csv'\n",
    "\n",
    "# URL for grid masking file\n",
    "blank_json = 'https://users.ugent.be/~pjdsmedt/ESS2023/blank.json'\n",
    "\n",
    "# Create dataframes from datasets\n",
    "'''\n",
    "Import datasets as dataframes\n",
    "-----------------------------\n",
    "    - df = dataframe with the full FDEM dataset\n",
    "    - dt = dataframe with the FDEM transect\n",
    "    - ds = datasframe with the sample data (including analytical data)\n",
    "    - blank = geojson (polygon) outlining survey extent\n",
    "'''\n",
    "df = pd.read_csv(FDEM_surveydata, sep=',', header=0)\n",
    "dt = pd.read_csv(FDEM_transect, sep=',', header=0)\n",
    "ds = pd.read_csv(samples, sep=',', header=0)\n",
    "blank_in = gpd.read_file(blank_json)\n",
    "blank = blank_in.to_crs('EPSG:31370')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas dataframe names\n",
    "\n",
    "As presented in the code cell above, all datasets are loaded as pandas dataframes. You can access the column data through the column names, as is the case throughout this notebook.\n",
    "If you want to quickly view the column names, just create a code cell, and type the dataframe name with the suffix *.colums* ( like this: `df.columns`) and run the cell, you can run **_code cell 0.4_** as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.4: Printing dataframe column names\n",
    "# ------------------------------------\n",
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Interpolating and exporting data.\n",
    "\n",
    "In this notebook, you will work with geospatial data in vector (point) and raster format. To help you along the way to analyse and visualise data, in **_code cell 1.0_**, basic functions to interpolate and export data are given. You can call these functions anywhere in the notebook, as long as you run the following code cell first. \n",
    "When working in Google Colaboratory, running the export function anywhere in the notebook will store the exported geotif on your Google Drive.  \n",
    "\n",
    "**!!! If you export two different datasets with the same filename, the first dataset will be overwritten !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0: Functions for interpolating and exporting data\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# a. Function to quickly interpolate a scatter dataset to a regular grid.\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "def interpolate(x, y, z, cell_size, method='nearest', \n",
    "                smooth_s = 0, blank=blank):\n",
    "    \"\"\"\n",
    "    Interpolate scatter data to regular grid through selected interpolation \n",
    "    method (with scipy.interpolate for simple interpolation).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.array\n",
    "        Cartesian GPS x-coordinates.\n",
    "\n",
    "    y : np.array\n",
    "        Cartesian GPS y-coordinates.\n",
    "\n",
    "    z : np.array\n",
    "        Data points to interpolate.\n",
    "\n",
    "    cell_size : float\n",
    "        Grid cell size (m).\n",
    "\n",
    "    method : str, optional\n",
    "        Scipy interpolation method ('nearest', 'linear', 'cubic' or 'IDW')\n",
    "\n",
    "    smooth_s : float, optional\n",
    "        Smoothing factor to apply a Gaussian filter on the interpolated grid.\n",
    "        If 0, no smoothing is performed. (Applying smoothing can result \n",
    "        in a loss of detail in the interpolated grid.)\n",
    "\n",
    "    blank : object\n",
    "        A blank object to mask (clip )interpolation beyond survey bounds.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grid : np.array\n",
    "        Array of interpolated and masked grid containing:\n",
    "        - the interpolated grid (grid['grid'])\n",
    "        - the grid cell size (grid['cell_size'])\n",
    "        - the grid extent (grid['extent'])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an evenly spaced grid over which the dataset values have to be \n",
    "    # interpolated\n",
    "    x_min = x.min()\n",
    "    x_max = x.max() + cell_size\n",
    "    y_min = y.min()\n",
    "    y_max = y.max() + cell_size\n",
    "    x_vector = np.arange(x_min, x_max, cell_size)\n",
    "    y_vector = np.arange(y_min, y_max, cell_size)\n",
    "    extent = (x_vector[0], x_vector[-1], y_vector[0], y_vector[-1])\n",
    "\n",
    "    xx, yy = np.meshgrid(x_vector, y_vector)\n",
    "    nx, ny = xx.shape\n",
    "    coords = np.concatenate((xx.ravel()[np.newaxis].T, \n",
    "                        yy.ravel()[np.newaxis].T), \n",
    "                        axis=1)\n",
    "\n",
    "    # Create a mask to blank grid outside surveyed area\n",
    "    boolean = np.zeros_like(xx)\n",
    "    boundaries = np.vstack(blank.loc[0, 'geometry'].exterior.coords.xy).T\n",
    "    bound = boundaries.copy()\n",
    "    boolean += matplotlib.path.Path(\n",
    "        bound).contains_points(coords).reshape((nx, ny))\n",
    "    boolean = np.where(boolean >= 1, True, False)\n",
    "    mask = np.where(boolean == False, np.nan, 1)\n",
    "    binary = np.where(boolean == False, 0, 1)\n",
    "    \n",
    "    # Fast (and sloppy) interpolation (scipy.interpolate)\n",
    "    if method in ['nearest','cubic', 'linear']:\n",
    "        # Interpolate \n",
    "        data_grid = griddata(\n",
    "            np.vstack((x, y)).T, z, (xx, yy), method=method\n",
    "            ) * mask\n",
    "    else:\n",
    "        print('define interpolation method')\n",
    "    \n",
    "    if smooth_s > 0:\n",
    "        data_grid = gaussian_filter(data_grid, sigma=smooth_s)\n",
    "\n",
    "    # Create a structured array with additional fields for coordinates and cell size\n",
    "    dtype = [\n",
    "        ('grid', data_grid.dtype, data_grid.shape),\n",
    "        ('cell_size', float),\n",
    "        ('extent', [\n",
    "            ('x_min', float), \n",
    "            ('x_max', float), \n",
    "            ('y_min', float), \n",
    "            ('y_max', float)\n",
    "            ])\n",
    "    ]\n",
    "    grid = np.array((data_grid, cell_size, extent), dtype=dtype)\n",
    "    \n",
    "    return grid\n",
    "  \n",
    "\n",
    "# b. Function to export an interpolated grid as a geotif.\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def export_grid(grid_in, filename='georaster'):\n",
    "    \"\"\"\n",
    "    Interpolate scatter data to regular grid through selected interpolation \n",
    "    method (with scipy.interpolate for simple interpolation).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grid_in : np.array\n",
    "        Array of interpolated and masked grid.\n",
    "\n",
    "    filename : str, optional\n",
    "        Name of the GeoTIFF (.tif) file (standard = 'gridded').\n",
    "\n",
    "    y : np.array\n",
    "        Cartesian GPS y-coordinates.\n",
    "\n",
    "    z : np.array\n",
    "        Data points to interpolate.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #   Get grid properties\n",
    "    cell_size = grid_in['cell_size']\n",
    "    extent = grid_in['extent']\n",
    "    transform = from_origin(extent['x_min'], extent['y_min'], \n",
    "                                cell_size, -cell_size)\n",
    "\n",
    "    #   Prepare rasterio grid\n",
    "    grid_exp = grid_in['grid']\n",
    "    grid_exp[np.isnan(grid_exp)] = -99999\n",
    "    grid_exp = grid_exp.astype(rasterio.float32)\n",
    "    nx, ny = grid_exp.shape\n",
    "    grid_exp = np.flip(grid_exp, axis=0)\n",
    "\n",
    "    \n",
    "    #   Create an empty grid with correct name and coordinate system\n",
    "    with rasterio.open(\n",
    "        filename + '.tif',\n",
    "        mode='w',\n",
    "        driver='GTiff',\n",
    "        height=nx,\n",
    "        width=ny,\n",
    "        count=1,\n",
    "        dtype=str(grid_exp.dtype),\n",
    "        crs='EPSG:31370', #Lambert 1972 coordinates\n",
    "        transform=transform,\n",
    "        nodata=-99999\n",
    "    ) as dst:\n",
    "        dst.write(grid_exp, 1)\n",
    "\n",
    "    # Open the GeoTIFF file in read/write mode to flip the image vertically\n",
    "    with rasterio.open(filename + '.tif', mode='r+') as dst:\n",
    "        data = dst.read()\n",
    "        dst.write(data[0, ::-1], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of functions\n",
    "\n",
    "In the code cells below, you can use the `interpolate` function to interpolate a single dataset from the FDEM dataset collected on 28.04.2023. The output of this function is a Numpy array that holds the interpolated data (accessed via `datagrid['grid']` in the cell below), alongside the grid's cell size (`datagrid['cell_size']`) and the grid extent (`datagrid['extent']`). The cell size and extent are needed to allow exporting the interpolated data efficiently to a GeoTIF that can be opened in any GIS software such as QGIS. \n",
    "\n",
    "**_Code cell 1.1_** allows interpolating and plotting a single dataset. You can do this for all datasets, by specifying the data column through the `col` variable. If you run this notebook for the first time, the `col` variable will be set to `HCP1.0`. This will result in the interpolation of the ECa data collected with the 1.0 m HCP coil configuration. You can change it to any of the data column names of the FDEM survey dataset, shown in *Table 1*. The second variable to set is the grid cell size (`cell_size` variable), which specifies the resolution of the final grid in meters.\n",
    "\n",
    "In **_code cell 1.2_** you can export the interpolated dataset as a GeoTIF for use in your preferred GIS program. You can reuse these functions across the notebook, to interpolate datasets and export generated rasters.  \n",
    "\n",
    "**_Code cell 1.3_** is a more advanced example of how you an iteratively interpolate and plot all datasets from the FDEM survey. Running this cell provides an overview of all collected ECa and IP datasets.\n",
    "\n",
    "Take note that the interpolation algorithms implemented here have the primary purpose of quickly visualising data. These are very simple interpolators, from the [SciPy](https://docs.scipy.org/doc/scipy/tutorial/interpolate.html) package, that do not take geostatistical relationship or distances between points into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1: Example of interpolating and plotting a single FDEM dataset\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# Specify the data column of which you want to interpolate the values\n",
    "col = 'HCP1.0'\n",
    "cell_size = 0.25 # raster cell size in meters\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# Interpolation function\n",
    "data_grid = interpolate(df['x'], df['y'], df[col], cell_size=0.25)\n",
    "\n",
    "# Specify the grid extent for plotting with correct x-y coordinates\n",
    "extent = data_grid['extent']\n",
    "\n",
    "# Set units and colormap (cmap) for either IP or ECa data\n",
    "if 'inph' in col:\n",
    "        unit = 'IP [ppt]'\n",
    "        cmap = 'gray_r'\n",
    "else:\n",
    "        unit = 'ECa [mS/m]'\n",
    "        cmap = 'viridis_r'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "im = ax.imshow(data_grid['grid'], \n",
    "                origin='lower', \n",
    "                extent=(extent['x_min'],\n",
    "                        extent['x_max'],\n",
    "                        extent['y_min'],\n",
    "                        extent['y_max']),\n",
    "                cmap = 'viridis_r'\n",
    "                )\n",
    "# Set limits to the plotting range based on data percentiles by \n",
    "# uncommenting the 4 lines below: \n",
    "\n",
    "# pmin = 2  # lower percentile\n",
    "# pmax = 98  # upper percentile \n",
    "# im.set_clim(np.percentile(data_grid['grid'].flatten()[~np.isnan(data_grid['grid'].flatten())], pmin),\n",
    "#         np.percentile(data_grid['grid'].flatten()[~np.isnan(data_grid['grid'].flatten())], pmax))\n",
    "\n",
    "ax.set_title(f\"{col} ({unit})\")\n",
    "plt.colorbar(im, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2: Running this cell exports the interpolated dataset as a GeoTIF.\n",
    "# --------------------------------------------------------------------\n",
    "\"\"\"\n",
    "You can set the exported file's name by modifying the f_name variable.\n",
    "In Google Colaboratory, this will be exported to your Google drive, after\n",
    "which you can download the data locally.\n",
    "\"\"\"\n",
    "\n",
    "f_name = '1mHCP_ECa' # Filename (.tif suffix is added automatically)\n",
    "\n",
    "# ******************************************************************** #\n",
    "export_grid(data_grid, filename = f_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3: Example on interpolating and plotting via loop and visualise all datasets\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 15))\n",
    "\n",
    "i = 0\n",
    "data_grids = {}\n",
    "for col in df.columns:\n",
    "    # Omit columns that do not contain measurement data\n",
    "    if col not in ['x','y','z','t']:\n",
    "        row, col_idx = divmod(i, 3)\n",
    "        ax = axes[row, col_idx]\n",
    "\n",
    "        # Set plotting properties for inphase and ECa data\n",
    "        if '_inph' in col:\n",
    "            cmap = 'gray_r'\n",
    "            unit = 'ppt'\n",
    "        else:\n",
    "            cmap = 'viridis_r'\n",
    "            unit = 'mS/m'\n",
    "\n",
    "        # Interpolate the survey dataset\n",
    "        data_grid = interpolate(df['x'], df['y'], df[col],\n",
    "                                        cell_size=0.5)\n",
    "        extent = data_grid['extent']\n",
    "        gridplot = data_grid['grid']\n",
    "\n",
    "        # Visualise the results\n",
    "        im = ax.imshow(\n",
    "                        gridplot, \n",
    "                        origin='lower', \n",
    "                        extent=(extent['x_min'],\n",
    "                                extent['x_max'],\n",
    "                                extent['y_min'],\n",
    "                                extent['y_max']),\n",
    "                        cmap = cmap\n",
    "                        )\n",
    "        ax.set_title(f\"{col} ({unit})\")\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        i += 1\n",
    "df_data_grids = pd.DataFrame.from_dict(data_grids)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. FDEM data exploration\n",
    "### Exploring data in QGIS\n",
    "\n",
    "With the above functions, you are all set to start exploring the FDEM data. If you have exported one or more of the FDEM datasets as geotifs, you can open these in a GIS (instructions here are for [QGIS](https://www.qgis.org/en/site/)) and compare the observed electromagnetic variations to known soil information.\n",
    "In QGIS, and create a new project (Ctrl+N, or by using the menus). Set the coordinate reference system to Belgian Lambert 1972 (EPSG:31370).\n",
    "\n",
    "Raster layers can be added by pressing Ctrl+Shift+R, or navigating to the `Layer > Add Layer > Add Raster Layer` menu item.\n",
    "You can also add the delimited text file (.csv-file) containing the sample data by pressing Ctrl+Shift+T or navigating to the `Layer > Add Layer > Add Delimited Text Layer` menu item. Make sure that you specific the columns with x- and y- coordinates correctly (X field = `x`; Y field = `y`).\n",
    "\n",
    "You can add the Flemish soil map to your project by adding the WMS layer. \n",
    "Do this by pressing CTRL-Shift-W (or via `Layer>Add Layer>Add WMS/WMTS Layer`). In the popup box you create a new service connection by pressing 'New'. You can enter a name to identify the layer (e.g. 'SoilMap'), and in the URL you copy this address: https://www.dov.vlaanderen.be/geoserver/bodemkaart/bodemtypes/wms?version=1.3.0&request=GetCapabilities&service=wms \n",
    "\n",
    "Then press 'Connect' and you should see the different layers in this WMS repository. If you select 1 -- bodemtypes, and Add this (click the button below), you have added the soil map to your project. You can explore the other layers in the WMS as well. Mainly the Digital Soil Map (1) and the drainage class map (2) are relevant for this study. \n",
    "\n",
    "The names are indicated in Dutch, but here you have them translated:\n",
    "\n",
    "1. soil types - Digital Soil Map of the Flemish Region ('bodemtypes')\n",
    "2. drainage classes (labeled)\n",
    "3. soil series (labeled)\n",
    "4. drainage classes (unlabeled)\n",
    "5. soil series (simplified legend)\n",
    "6. soil series (contours)\n",
    "7. soil series (unlabeled)\n",
    "\n",
    "This way, you can compare the spatial patterning in the geophysical data to the information from the soil map, and to the sample information. You can use this QGIS project to combine and visualise all data from your project.\n",
    "\n",
    "### 2.1 Evaluating variability in FDEM datasets\n",
    "\n",
    "Next, you can explore the variability in the FDEM data more quantitatively. In **_code cell 2.1_**, you have a function that allows generating the relative sensitivity functions of the deployed coil pairs. This allows you to evaluate at which depth the coil geometry obtains its maximum sensitivity to changes in subsurface electrical conductivity (for the QP (or LIN ECa) response).\n",
    "\n",
    "In **_code cell 2.2_**, you have some plotting functions that allow a first more detailed evaluation of the FDEM datasets. You have a plot of the ECa data collected along the reference transect (dataframe `dt`), alongside a plot of the QP sensitivities of all deployed coil geometries. This provides more detailed insight into the spatial sensitivity of the used configurations, and illustrates at which depths below the surface, most information is gathered with a given coil pair.\n",
    "In the last plot, the ECa values of the HCP1.0 coil pair are shown at sampling locations 1 - 15. In this scatterplot, the color coding and size of the scatter points are determined by the analytical data obtained at these sampling locations. You can modify the plot parameters to evaluate different relationships and explore the data in more detail. \n",
    "\n",
    "To evaluate the data in more detail, you can evaluate the histograms and statistics of the full dataset (dataframe `df`), for which example code is provided in the introductory notebook). \n",
    "To investigate the relationships between the observed sensor data (FDEM data) and the analytical data at the sampling locations, you can explore the `ds` dataframe, which holds all analytical information and the nearest sensor observations at each of the 15 sampling locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1: Function to obtain the sensitivity of a given coil geometry\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "def lin_sens(geometry, all=False):\n",
    "    \"\"\"\n",
    "    Calculate approximative cumulative and relative sensitivities based on\n",
    "    Keller & Frischknecht, 1966 and McNeill, 1980\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    geometry : str\n",
    "        coil geometry identifier, combining orientation ('HCP' or 'PRP'),\n",
    "        and Tx-Rx separation. 'HCP0.5'is thus a HCP orientation with a 0.5 m\n",
    "        coil separation.\n",
    "\n",
    "    all : boolean, optional\n",
    "        if set to true, the function returns the cumulative and relative\n",
    "        sensitivity of the QP (ECa) and IP response of the evaluated coil pair.\n",
    "        If set to false, only the relative sensitivity of the QP response is \n",
    "        returned.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rsens_QP : np.array\n",
    "        Array of relative QP sensitivities obtained over the evaluated depths.\n",
    "    \n",
    "    csens_QP : np.array, optional\n",
    "        Array of cumulative QP sensitivities obtained over the evaluated depths.\n",
    "\n",
    "    rsens_IP : np.array, optional\n",
    "        Array of relative IP sensitivities obtained over the evaluated depths.\n",
    "    \n",
    "    csens_IP : np.array, optional\n",
    "        Array of cumulative IP sensitivities obtained over the evaluated depths.\n",
    "\n",
    "    \"\"\"\n",
    "    # determine depth extent along which to evaluate sensitivity\n",
    "    depths = np.linspace(.0, 2.0, 100)\n",
    "    sensor_height = 0.165\n",
    "    if 'inph' in geometry:\n",
    "        coil_spacing = float(geometry[3:6])\n",
    "    else: \n",
    "        coil_spacing = float(geometry[-3:])\n",
    "\n",
    "    # create empty arrays\n",
    "    csens_QP = np.empty_like(depths)\n",
    "    rsens_QP = np.empty_like(depths)\n",
    "    csens_IP = np.empty_like(depths)\n",
    "    rsens_IP = np.empty_like(depths)\n",
    "    \n",
    "    depth_ratio = (depths + sensor_height) / coil_spacing\n",
    "    if 'HCP' in geometry:\n",
    "        csens_IP = (1 - 8 * depth_ratio ** 2) / ((4 * (depth_ratio ** 2) + 1) ** (5 / 2))\n",
    "        rsens_IP = 12 * depth_ratio * (3 - 8 * depth_ratio ** 2) / (coil_spacing * ((4 * depth_ratio ** 2) + 1) ** (7 / 2))\n",
    "        csens_QP = 1 / (4 * (depth_ratio ** 2) + 1) ** 0.5\n",
    "        rsens_QP = 4 * depth_ratio / (coil_spacing * (4 * depth_ratio ** 2 + 1) ** (3 / 2))\n",
    "\n",
    "    if 'PRP' in geometry:\n",
    "        csens_IP = (6 * depth_ratio) / ((4 * (depth_ratio ** 2) + 1) ** 2.5)\n",
    "        rsens_IP = -(96 * (depth_ratio ** 2) - 6) / (coil_spacing * (4 * (depth_ratio ** 2) + 1) ** (7 / 2))\n",
    "        csens_QP = 1 - (2 * depth_ratio) / ((4 * depth_ratio ** 2) + 1) ** 0.5\n",
    "        rsens_QP = 2 / ((coil_spacing * (4 * depth_ratio ** 2) + 1) ** (3 / 2))\n",
    "    if all:\n",
    "        return rsens_QP, csens_QP, rsens_IP, csens_IP\n",
    "    else:\n",
    "        return rsens_QP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2: Evaluating FDEM data along the reference transect\n",
    "# ------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Here you can compare plotted ECa data along the reference transect, \n",
    "and evaluate these at the sampling locations. In addition, you can \n",
    "evaluate the relative sensitivity of the QP (LIN ECa) response of the \n",
    "deployed coil configurations. \n",
    "------------------------------------------------------------------------\n",
    "For the third plot, you can evaluate the analytical data at the sampling \n",
    "locations (sample_col), and compare these to a selected FDEM dataset (fdem_col)\n",
    "You can select the desired variables in the two lines of code below this \n",
    "comment. You can change these strings to visually evaluate the relationships\n",
    "# between the analysed properties and the output from different coil pairs.\n",
    "\"\"\"\n",
    "\n",
    "sample_col = 'vwc [%]' # change into, e.g., 'vwc [%]' or 'bd  [g/cm3]'\n",
    "fdem_col = 'HCP1.0' # change into, for instance, 'PRP1.0' or 'HCP2.0_inph'\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(21,7))\n",
    "ax1 = axes[0]\n",
    "ax2 = axes[1]\n",
    "ax3 = axes[2]\n",
    "for col in dt.columns:\n",
    "    if col not in ['x','y','z','t','pos'] and 'inph' not in col:\n",
    "        if 'PRP' in col:\n",
    "            ax1.plot(dt[col], linestyle = 'dashed', label = col)\n",
    "        else:\n",
    "            ax1.plot(dt[col], label = col)\n",
    "ax1.set_ylabel('ECa in mS/m')\n",
    "ax1.set_title('ECa data along reference transect (0 = North).')\n",
    "ax1.legend()\n",
    "\n",
    "depths = np.linspace(.0, 2.0, 100)\n",
    "for col in dt.columns:\n",
    "    if col not in ['x','y','z','t','pos'] and 'inph' not in col:\n",
    "        if 'PRP' in col:\n",
    "            ax2.plot(lin_sens(col), depths, linestyle = 'dashed', label=col)\n",
    "        else:\n",
    "            ax2.plot(lin_sens(col), depths, label=col)\n",
    "\n",
    "ax2.invert_yaxis()\n",
    "ax2.xaxis.tick_top()\n",
    "ax2.xaxis.set_label_position('top')\n",
    "ax2.set_xlabel('Relative Sensitivity [-]')\n",
    "ax2.set_ylabel('Depth [m]')\n",
    "ax2.legend()\n",
    "\n",
    "point_sizes = 220  # set marker size\n",
    "norm = plt.Normalize(ds[sample_col].min(), ds[sample_col].max())\n",
    "\n",
    "scatter = ax3.scatter(ds[sample_col], ds[fdem_col], \n",
    "                        s=point_sizes)\n",
    "\n",
    "ax3.set_xlabel(sample_col)\n",
    "if 'inph' in fdem_col:\n",
    "    ax3.set_ylabel(fdem_col + ' [ppt]')\n",
    "    ax3.set_title(f'{fdem_col} IP response per sample location versus {sample_col}.')\n",
    "else:\n",
    "    ax3.set_ylabel(fdem_col + ' [mS/m]')\n",
    "    ax3.set_title(f'{fdem_col} ECa per sample location versus {sample_col}.')\n",
    "\n",
    "# Add labels to scatter points showing the sample ID's\n",
    "for index, row in ds.iterrows():\n",
    "    ax3.text(row[sample_col], row[fdem_col], str(int(row['ID'])), \n",
    "            fontsize=10, ha='center', va='center', color='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploring relationships between target properties and observed geophysical variations.\n",
    "\n",
    "To evaluate to which extent the collected FDEM data can help predict the spatial variability of your group's target properties, it is important to understand the relationship between the sensor output, and the targeted properties. This can be done at the sampling locations by further exploring the `ds` dataframe that contains both the FDEM data and analytical data at these locations.\n",
    "For instance, you can compare the FDEM ECa data to a given property at a given depth, and make the same comparision with the Hydraprobe EC data.\n",
    "\n",
    "## Stochastic approach\n",
    "One way to evaluate the relationship between targets and geophysical properties is through a stochastic approach. As an example, regressions are included in **_code cell 3.0_**. Code for both a linear and polynomial regression is included, but only the linear regression is performed (the polynomial code is commented out).\n",
    "The obtained linear function is incorporated in the 'lin_fit' object. You can obtain the function by simply printing the objects as: `print(lin_fit)`, as is done in **_code cell 3.0_**. \n",
    "\n",
    "To extract the slope and intercept from the linear model, you can directly access these as:\n",
    "\n",
    "`slope = lin_fit[1]` <br />\n",
    "`intercept = lin_fit[0]`,\n",
    "\n",
    "Suited models, that avoid overfitting and provide satisfactory prediction errors, can then be used to predict the target variable based on the obtained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0: evaluating the relationship between a target property \n",
    "# ----------------------------------------------------------\n",
    "\n",
    "target = 'clay [%]' # ds columnname for column with target property \n",
    "fdem_col = 'HCP1.0' # ds columnname for column with FDEM dataset \n",
    "hydra_col = 'hydra_ec [mS/m]' # ds columnname for hydraprobe data column\n",
    "\n",
    "\n",
    "\n",
    "# create a copy of the sample dataset to plot and, if needed, filter out\n",
    "# specific points\n",
    "ds_in = ds.copy()\n",
    "\n",
    "# # uncomment these lines to remove the validation or other points\n",
    "# exclude_points = [11, 12, 13, 14, 15] # replace with point ID's you want to remove\n",
    "# ds_in = ds_in[~ds_in['ID'].isin(exclude_points)]\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Perform a linear (and if desired polynomial) regression (cf. notebook 1)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# 'X' is the input variable of feature\n",
    "# 'y' is the output variable or 'target'to be predicted\n",
    "\n",
    "X = ds_in[fdem_col].values.reshape(-1, 1) \n",
    "y = ds_in[target].values.reshape(-1, 1) \n",
    "\n",
    "sorted_indices = np.argsort(X, axis=0).flatten()\n",
    "X_sorted = X[sorted_indices]\n",
    "y_sorted = y[sorted_indices]\n",
    "\n",
    "# Perform a linear regression with Numpy (np)\n",
    "lin_fit = np.poly1d(np.polyfit(ds_in[fdem_col], ds_in[target], 1))\n",
    "lin_pred = lin_fit(X_sorted)\n",
    "\n",
    "# # Perform a polynomial regression with Numpy (np)\n",
    "# # set the polynomial degree\n",
    "# poly_degree = 2 # polynomial degree\n",
    "# poly_fit = np.poly1d(np.polyfit(ds_in[fdem_col], ds_in[target], poly_degree))\n",
    "# poly_pred = poly_fit(X_sorted)\n",
    "# y_poly_pred = poly_fit(X_sorted)\n",
    "# ply_score = r2_score(y_sorted, poly_pred)\n",
    "\n",
    "# Get the coefficient of determination (R-squared) of the linear regressions\n",
    "lin_score = r2_score(y_sorted, lin_pred)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Plotting\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(21,7))\n",
    "# Define different axes for plotting\n",
    "ax1 = axes[0]\n",
    "ax2 = axes[1]\n",
    "ax3 = axes[2]\n",
    "\n",
    "# First plot\n",
    "ax1.scatter(ds_in[fdem_col],ds_in[target])\n",
    "ax1.set_xlabel(f'{fdem_col} data')\n",
    "ax1.set_ylabel(f'{target}')\n",
    "\n",
    "# add sample ID labels to the plot\n",
    "for index, row in ds_in.iterrows():\n",
    "    ax1.text(row[fdem_col], row[target], str(int(row['ID'])), \n",
    "    fontsize=10, ha='right', va='bottom')\n",
    "\n",
    "# Second plot\n",
    "ax2.scatter(ds_in[hydra_col],ds_in[target])\n",
    "ax2.set_xlabel(hydra_col)\n",
    "ax2.set_ylabel(f'{target}')\n",
    "\n",
    "#   add sample ID labels to the plot\n",
    "for index, row in ds_in.iterrows():\n",
    "    ax2.text(row[hydra_col], row[target], str(int(row['ID'])), \n",
    "    fontsize=10, ha='center', va='bottom')\n",
    "\n",
    "# Third plot\n",
    "ax3.scatter(X_sorted, y_sorted, color='gray')\n",
    "ax3.plot(X_sorted, lin_pred, color='red', linewidth=3, \n",
    "            label=f'Linear, R2 = {lin_score:.3f}')\n",
    "# ax3.plot(X_sorted, y_poly_pred, color='blue', \n",
    "#             label=f'Polynomial, R2 = {ply_score:.3f}')\n",
    "ax3.set_xlabel(target)\n",
    "ax3.set_ylabel(fdem_col)\n",
    "ax3.legend()\n",
    "\n",
    "print(f'The obtained linear function is: {lin_fit}.')\n",
    "#print(f'The obtained polynomial function is: {poly_fit}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pedophysical modelling - predicting bulk EC\n",
    "\n",
    "Pedophysical models are models that aim to describe the relationship between the observed geophysical properties, and the soil properties and states of interest. Generally, a geophysical property of a soil (i.e., a medium that integrates multiple phases) is influenced by the soil constituents (i.e., the relative volumetric proportion of the soil components and their physical properties), and the soil structure (i.e., the spatial distribution of the soil constituents and their interconnection). In class, we have briefly discussed Archie's law, which, when modified for soils, primarily accounts for the conductivity of the pore fluid ($\\sigma_{w}$), and considers the degree of saturation $S_{w}$ alongside a saturation exponent $n$, which relates to the pore structure of the medium. The degree of saturation equals the ratio of the volumetric water content ($\\theta$) to the porosity ($\\phi$) of the medium:\n",
    "$$\n",
    "S_w = \\frac{\\theta}{\\phi}\n",
    "$$\n",
    "The combined Archie’s law is:\n",
    "$$\n",
    "\\sigma = \\phi^{m}S_{w}^{n}\\sigma_{w}.\n",
    "$$\n",
    "\n",
    "This allows estimating the conductivity $\\sigma$ for partially saturated media whereby $m$ (fixed at 1.5) and $n$ (fixed at 2) allow including the influence of the medium’s structure.\n",
    "\n",
    "One major shortcoming of Archie’s law was originally developed for sandy rocks, which presents significant shortcomings when applied to other media such as soils. Most importantly, surface conduction (i.e., the conduction along the surface of dry particles) is not taken into account. In all soils, the influence of surface conductance is non-negligible, and increases with clay content. Other models account for this, such as the adjustment proposed by [Waxman & Smits (1968)](https://users.ugent.be/~pjdsmedt/ESS2023/WS_1986.pdf), and the equation from [Linde et al. 2006](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2006WR005131) mentioned in [Romero-Ruiz et al. 2018](http://doi.wiley.com/10.1029/2018RG000611):\n",
    "\n",
    "$$\n",
    "\\sigma = \\phi^{m}S_{w}^{n}\\sigma_{w} + (1 - \\phi^{m})\\sigma_{surface}.\n",
    "$$\n",
    "\n",
    "As the surface conductivity is a complex property to quantify, it is often estimated based on related soil properties such as the clay content. The equation of Linde et al. 2006 determines surface conductivity based on soil texture, by attributing more weight to a set conductivity of solid particles as the particle size decreases. \n",
    "\n",
    "This equation is provided in **_code cell 3.1_** as `linde`. You can test this equation for a range of varying soil properties in **_code cell 3.2_**. \n",
    "  \n",
    "**IMPORTANT: In this functions, the electrical conductivities (of bulk soil, pore water, and surface conductivity) are outputted, and have to be inputted, in Siemens per meter (S/m).** This means you have to account for this when inputting values in mS/m, and reading the functions' output. All conductivity data in the datasets you have are presented in mS/m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1: Pedophysical modelling - Linde et al. 2006\n",
    "# -----------------------------------------------\n",
    "# def linde(vwc, bd, clay, water_ec, sand, silt, pdn=2.65, m=1.5, n=2):\n",
    "def linde(vwc, bd, sand, clay, water_ec, pdn=2.65, m=1.5, n=2):\n",
    "    \"\"\"        \n",
    "        Parameters\n",
    "        ----------\n",
    "        vwc: float\n",
    "            volumetric water content [-]\n",
    "        \n",
    "        bd: float\n",
    "            bulk density [g/cm3]\n",
    "\n",
    "        clay: float\n",
    "            Soil volumetric clay content [%]\n",
    "\n",
    "        water_ec: float\n",
    "            Soil water real electrical conductivity [mS/m]\n",
    "\n",
    "        pdn: float\n",
    "            particle density [g/cm3]\n",
    "\n",
    "        m: float\n",
    "            cementation exponent [-]\n",
    "\n",
    "        n: float\n",
    "            saturation exponent [-]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bulk_ec: float\n",
    "            Soil bulk real electrical conductivity [mS/m]\n",
    "    \"\"\"  \n",
    "\n",
    "    por = 1-(bd/pdn) # porosity\n",
    "    sat_w = (vwc/100)/por # water saturation\n",
    "    f_form = por**(-m) # formation factor\n",
    "    \n",
    "    water_ec = water_ec/1000\n",
    "\n",
    "    silt = 100 - clay - sand\n",
    "\n",
    "    radius_clay = 0.002/2000\n",
    "    radius_silt = 0.025/2000\n",
    "    radius_sand = 0.75/2000\n",
    "\n",
    "    solid_ec = 1*(10**-7) # Solid electrical conductivity\n",
    "    clay_ec= 3*(solid_ec/radius_clay)  # clay electrical conductivity\n",
    "    silt_ec = 3*(solid_ec/radius_silt) # Silt electrical conductivity\n",
    "    sand_ec = 3*(solid_ec/radius_sand) # Sand electrical conductivity\n",
    "\n",
    "    surf_ec = np.average([clay_ec*(clay/100), \n",
    "                          sand_ec*(sand/100), \n",
    "                          silt_ec*(silt/100)])\n",
    "    bulk_ec = (((sat_w**n)*water_ec) \n",
    "               + (f_form - 1)*(surf_ec))/f_form \n",
    "    \n",
    "    return bulk_ec*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3: Simple simulation that shows the influence of changing clay content \n",
    "# and bulk density on the soil electrical conductivity  (bulk_ec)\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Here you can evaluate the Linde et al. (2006) equation for predicting soil conductivity.\n",
    "You can also set the range of clay content and bulk density to input into the\n",
    "models, by setting the minimum and maximum values.\n",
    "\"\"\"\n",
    "\n",
    "# Set minimum and maximum clay content [%], bulk density [g/cm**3] and \n",
    "# volumetric water content [%]\n",
    "\n",
    "min_clay = 8\n",
    "max_clay = 40\n",
    "\n",
    "min_bd = 1\n",
    "max_bd = 2.1\n",
    "min_vwc = 5\n",
    "max_vwc = 35\n",
    "\n",
    "# setting fixed values for vwc, ec_water, clay, and bd as appropriate \n",
    "vwc_f = 10\n",
    "ec_water_f = 50 #mS/m\n",
    "clay_f = 40\n",
    "b_dens_f = 1.25\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# getting clay and bd range\n",
    "\n",
    "clay_i = np.linspace(min_clay, max_clay, 100)\n",
    "b_dens_i = np.linspace(min_bd, max_bd, 100)\n",
    "vwc_i = np.linspace(min_vwc, max_vwc, 100)\n",
    "\n",
    "# create empty list to populate with iteration results\n",
    "b_it = []\n",
    "c_it = []\n",
    "v_it = []\n",
    "\n",
    "# iterate over bulk densities\n",
    "for i in range(100):\n",
    "    s_in = (100-clay_f)/2\n",
    "    bulk_ec=linde(vwc_f, \n",
    "                    b_dens_i[i], \n",
    "                    s_in,  \n",
    "                    clay_f, \n",
    "                    ec_water_f)\n",
    "\n",
    "    b_it.append([bulk_ec,b_dens_i[i]])\n",
    "\n",
    "# iterate over clay contents\n",
    "for i in range(100):\n",
    "    s_in = (100-clay_i[i])/2\n",
    "    bulk_ec=linde(vwc_f, \n",
    "                    b_dens_f,\n",
    "                    s_in, \n",
    "                    clay_i[i],\n",
    "                    ec_water_f)\n",
    "\n",
    "    c_it.append([bulk_ec,clay_i[i]])\n",
    "\n",
    "# iterate over volumetric water content\n",
    "for i in range(100):\n",
    "    s_in = (100-clay_f)/2\n",
    "    bulk_ec = linde(vwc_i[i], \n",
    "                    b_dens_f,\n",
    "                    s_in, \n",
    "                    clay_f, \n",
    "                    ec_water_f)\n",
    "\n",
    "    v_it.append([bulk_ec, vwc_i[i]])\n",
    "\n",
    "ec_bd = np.asarray(b_it)\n",
    "ec_cl = np.asarray(c_it)\n",
    "ec_vwc = np.asarray(v_it)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=[18, 5])\n",
    "axes[0].scatter(ec_bd[:,1], ec_bd[:,0],\n",
    "            alpha=.7)\n",
    "axes[1].scatter(ec_cl[:,1], ec_cl[:,0],\n",
    "            alpha=.7)\n",
    "axes[2].scatter(ec_vwc[:, 1], ec_vwc[:, 0],\n",
    "                alpha=.7)\n",
    "axes[0].set_xlabel(\"$b_d$ [g/$m^3$]\")\n",
    "axes[0].set_ylabel(\"$\\sigma_{soil}$ [mS/m]\")\n",
    "axes[0].set_title(f\"{clay_f}% clay, \"+ f\"vwc = {vwc_f}% \"+ \"$\\sigma_{water}$\" + f\"= {ec_water_f} mS/m\")\n",
    "axes[1].set_xlabel(\"clay content [%]\")\n",
    "axes[1].set_ylabel(\"$\\sigma_{soil}$ [mS/m]\")\n",
    "axes[1].set_title(\"$b_d$ = 1.25, \" + f\"vwc = {vwc_f}% \"+  f\"= {ec_water_f} mS/m\")\n",
    "axes[2].set_xlabel(\"volumetric water content [%]\")\n",
    "axes[2].set_ylabel(\"$\\sigma_{soil}$ [mS/m]\")\n",
    "axes[2].set_title(f\"{clay_f}% clay, \" + f\"$b_d$ = {b_dens_f} g/$m^3$, \" + \"$\\sigma_{water}$\" + f\"= {ec_water_f} mS/m\")\n",
    "\n",
    "fig.suptitle(f'Evaluation of Linde et al., 2006', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating sampling data\n",
    "\n",
    "In **_code cell 3.3_** you can evaluate the performance of these models by comparing the modelled soil EC values based on the analytical data, and comparing these to point EC data collected with the HydraProbe during fieldwork, and any of the FDEM datasets. Just specify the FDEM dataset to evaluate (again using the column names listed in *Table 1*). You can choose the fix the conductivity on the pore solution, to evaluate what impact this has on the prediction, by setting the `fix_ec_water` variable to `True`. This will use the average value of the pore solution conductivity as measured with the hydraprobe during fieldwork.  \n",
    "\n",
    "By running **_code_cell 3.4_** you can add the EC values modelled through the Linde et al. (2006) equation to the samples dataframe (dataframe `ds`), and exporting it as a csv file to your Google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3: Evaluating pedophysical relationships at sampling locations\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "fdem_col = 'HCP0.5'\n",
    "fix_ec_water = False\n",
    "ec_water_avg = ds['hydra_ecp [mS/m]'].mean()\n",
    "\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# Initialize empty lists to store the predicted bulk_ec values\n",
    "predicted_bulk_ec = []\n",
    "\n",
    "# Loop through the rows of the DataFrame\n",
    "for index, row in ds.iterrows():\n",
    "    # Get the input values from the DataFrame\n",
    "    vmc = row['vwc [%]']\n",
    "    bd = row['bd [g/cm3]']\n",
    "    sand = row['sand [%]']\n",
    "    # silt = row['silt [%]']\n",
    "    clay = row['clay [%]']\n",
    "    if fix_ec_water:\n",
    "        water_ec = ec_water_avg\n",
    "    else:\n",
    "        water_ec = row['hydra_ecp [mS/m]']\n",
    "    \n",
    "    # Call the Linde et al. 2006 equation function with the input values\n",
    "    bulk_ec = linde(vmc, bd, sand, clay, water_ec)\n",
    "    \n",
    "    # Store the predicted bulk_ec values\n",
    "    predicted_bulk_ec.append(bulk_ec)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[12, 5])\n",
    "\n",
    "# Plot bulk_ec versus hydra_ec [mS/m]\n",
    "axes[0].scatter(ds['hydra_ec [mS/m]'], predicted_bulk_ec, alpha=0.7)\n",
    "axes[0].set_xlabel(\"hydra_ec [mS/m]\")\n",
    "axes[0].set_ylabel(\"Predicted EC [mS/m]\")\n",
    "axes[0].set_title(\"Predicted EC vs hydra_ec\")\n",
    "#axes[0].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Plot bulk_ec versus HCP1.0\n",
    "axes[1].scatter(ds[fdem_col], predicted_bulk_ec, alpha=0.7)\n",
    "axes[1].set_xlabel(fdem_col)\n",
    "axes[1].set_ylabel(\"Predicted EC [mS/m]\")\n",
    "axes[1].set_title(f\"Predicted EC vs {fdem_col}\")\n",
    "#axes[1].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4: Store predicted EC values in the `ds` dataframe and save .csv\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Define a filename\n",
    "\n",
    "filename = 'samples_and_predictions.csv'\n",
    "\n",
    "# ******************************************************************** #\n",
    "# Create column for predicted EC values in the ds dataframe and export as csv\n",
    "ds['ec_linde [mS/m]'] = predicted_bulk_ec\n",
    "ds.to_csv(filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pedophysical modelling - predicting soil properties\n",
    "\n",
    "In **_code_cell 3.5_** functions are provided that allow predicting specific target properties based on an input EC value, and fixing the remaining parameters in the `linde` function.\n",
    "The functions allow prediction water content (`predict_water`), clay content (`predict_clay`), and bulk density (`predict_bd`). To perform the prediction you have to:\n",
    "- input an EC value;\n",
    "- fix the remaining parameters.\n",
    "\n",
    "For `predict_bd`, an example would be:\n",
    "```\n",
    "vwc = 40  #vol. water content = fixed at 10%\n",
    "sand = 35.03  #sand content = fixed at 35.03%\n",
    "silt = 42.13 #silt content = fixed at 42.03%\n",
    "clay = 22.84 #clay content = fixed at 22.84%\n",
    "water_ec = 197 #pore solution conductivity = fixed at 197 mS/m\n",
    "\n",
    "bulk_ec = [ec value here]\n",
    "\n",
    "bulk_dens_predicted = predict_bd(bulk_ec, vwc, sand, silt, clay, water_ec)\n",
    "\n",
    "```\n",
    "This would predict the bulk density, based on an input EC value in mS/m, while fixing the other parameters.\n",
    "In **_code_cell 3.5_** you have example code to do this on the entire sample dataframe (ds). In that example, all ECa values from the 'HCP1.0' column are used to predict bulk density. You can modify and reuse this code for the other prediction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5: Predicting water content, clay content and bulk density with Linde et al. (2006)\n",
    "# -------------------------------------------------------------------------------------\n",
    "def predict_water(bulk_ec, sand, clay, bd, water_ec):\n",
    "    wat_0 = 0\n",
    "    wat_end = 70\n",
    "    wat_step = 0.5\n",
    "    iterative_wat= np.arange(wat_0,  wat_end, wat_step)\n",
    "\n",
    "    linde_eval = linde(iterative_wat, bd, sand, clay, water_ec)\n",
    "\n",
    "    linde_diff = list(abs(linde_eval - [bulk_ec]*len(iterative_wat)))\n",
    "\n",
    "    wat_opt = iterative_wat[linde_diff.index(np.min(linde_diff))]\n",
    "    return wat_opt\n",
    "\n",
    "\n",
    "def predict_clay(bulk_ec, vwc, sand, bd, water_ec):\n",
    "    clay_0 = 6.5\n",
    "    clay_end = 50\n",
    "    clay_step = 0.5\n",
    "    iterative_clay= np.arange(clay_0, clay_end, clay_step)\n",
    "    # sand= (100-iterative_clay)/2\n",
    "    # silt= sand\n",
    "\n",
    "    linde_eval = linde(vwc, bd, sand, iterative_clay, water_ec)\n",
    "\n",
    "    linde_diff = list(abs(linde_eval - [bulk_ec]*len(iterative_clay)))\n",
    "\n",
    "    clay_opt = iterative_clay[linde_diff.index(np.min(linde_diff))]\n",
    "    return clay_opt\n",
    "\n",
    "\n",
    "def predict_bd(bulk_ec, vwc, sand, clay, water_ec):\n",
    "    bd_0 = 0.5\n",
    "    bd_end = 1.75\n",
    "    bd_step = 0.1\n",
    "    iterative_bd= np.arange(bd_0,  bd_end, bd_step)\n",
    "\n",
    "    linde_eval = linde(vwc, iterative_bd, sand, clay, water_ec)\n",
    "\n",
    "    linde_diff = list(abs(linde_eval - [bulk_ec]*len(iterative_bd)))\n",
    "\n",
    "    bd_opt = iterative_bd[linde_diff.index(np.min(linde_diff))]\n",
    "    return bd_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6a: Predicting water content for all rows in a dataframe\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "dpred_in = ds.copy() # If needed, you can replace ds.copy with another dataframe\n",
    "\n",
    "# Set the dataframe column name that holds the EC values \n",
    "ec_colname = 'hydra_ec [mS/m]' # or any other EC(a) dataset, e.g., 'HCP1.0' \n",
    "\n",
    "# Set to true if you want to fix the given properties for all datapoints\n",
    "fixed = False\n",
    "\n",
    "sand_f = dpred_in['sand [%]'].mean() #sand content\n",
    "clay_f = dpred_in['clay [%]'].mean() #clay content\n",
    "bd_f = dpred_in['bd [g/cm3]'].mean() #bulk density\n",
    "water_ec_f = dpred_in['hydra_ecp [mS/m]'].mean() #ec_water\n",
    "\n",
    "# NOTE: you can get the mean or median value of all values in a dataframe \n",
    "# column by:\n",
    "# - ds[column_name].mean()\n",
    "# - ds[column_name].median()\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "vwc_predicted = []\n",
    "\n",
    "for index, row in dpred_in.iterrows():\n",
    "    bulk_ec = row[ec_colname]\n",
    "    if not fixed:\n",
    "        sand = row['sand [%]']\n",
    "        clay = row['clay [%]']\n",
    "        bd = row['bd [g/cm3]']\n",
    "        water_ec = row['hydra_ecp [mS/m]']\n",
    "    else: \n",
    "        sand = sand_f\n",
    "        clay = clay_f\n",
    "        bd = bd_f\n",
    "        water_ec = water_ec_f\n",
    "\n",
    "    vwc_pred = predict_water(bulk_ec, sand, clay, bd, water_ec) #(bulk_ec, sand, clay, bd, water_ec)\n",
    "    vwc_predicted.append(vwc_pred)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=[12, 5])\n",
    "# Plot predicted property vs EC\n",
    "axes.scatter(dpred_in['vwc [%]'], vwc_predicted, alpha=0.7, c = 'blue')\n",
    "axes.set_xlabel(\"observed water content [%]\")\n",
    "axes.set_ylabel(\"Predicted water content [%]\")\n",
    "axes.set_title(f\"Observed vs predicted water content with {ec_colname}\")\n",
    "for index, row in dpred_in.iterrows():\n",
    "    axes.text(row['vwc [%]'], vwc_predicted[index], str(int(row['ID'])), \n",
    "            fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6b: Predicting clay content for all rows in a dataframe\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "dpred_in = ds.copy() # If needed, you can replace ds.copy with another dataframe\n",
    "\n",
    "# Set the dataframe column name that holds the EC values \n",
    "ec_colname = 'hydra_ec [mS/m]' # or any other EC(a) dataset, e.g., 'HCP1.0' \n",
    "\n",
    "# Set to true if you want to fix the given properties for all datapoints\n",
    "fixed = False\n",
    "\n",
    "vwc_f = dpred_in['vwc [%]'].mean()  \n",
    "sand_f = dpred_in['sand [%]'].mean()\n",
    "bd_f = dpred_in['bd [g/cm3]'].mean() \n",
    "water_ec_f = dpred_in['hydra_ecp [mS/m]'].mean() \n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "clay_predicted = []\n",
    "\n",
    "for index, row in dpred_in.iterrows():\n",
    "    bulk_ec = row[ec_colname]\n",
    "    if not fixed:\n",
    "        vwc = row['vwc [%]']\n",
    "        sand = row['sand [%]']\n",
    "        bd = row['bd [g/cm3]']\n",
    "        water_ec = ds['hydra_ecp [mS/m]'].mean() #\n",
    "    else:\n",
    "        vwc = vwc_f\n",
    "        sand = sand_f\n",
    "        bd = bd_f\n",
    "        water_ec = water_ec_f\n",
    "\n",
    "    clay_pred = predict_clay(bulk_ec, vwc, sand, bd, water_ec) #def predict_clay(bulk_ec, vwc, bd, water_ec):\n",
    "    clay_predicted.append(clay_pred)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=[12, 5])\n",
    "# Plot predicted property vs EC\n",
    "axes.scatter(dpred_in['clay [%]'], clay_predicted, alpha=0.7, c = 'green')\n",
    "axes.set_xlabel(\"observed clay content [%]\")\n",
    "axes.set_ylabel(\"Predicted clay content [%]\")\n",
    "axes.set_title(f\"Observed vs predicted clay content with {ec_colname}\")\n",
    "for index, row in dpred_in.iterrows():\n",
    "    axes.text(row['clay [%]'], clay_predicted[index], str(int(row['ID'])), \n",
    "            fontsize=10)\n",
    "#axes.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6c: Predicting bulk density for all rows in a dataframe\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "dpred_in = ds.copy() # If needed, you can replace ds.copy with another dataframe\n",
    "\n",
    "# Set the dataframe column name that holds the EC values \n",
    "ec_colname = 'hydra_ec [mS/m]' # or any other EC(a) dataset, e.g., 'HCP1.0' \n",
    "\n",
    "# Set to true if you want to fix the given properties for all datapoints\n",
    "fixed = False\n",
    "\n",
    "vwc_f = dpred_in['vwc [%]'].mean()  \n",
    "sand_f = dpred_in['sand [%]'].mean()\n",
    "clay_f = dpred_in['clay [%]'].mean() \n",
    "water_ec_f = dpred_in['hydra_ecp [mS/m]'].mean() \n",
    "\n",
    "# NOTE: you can get the mean or median value of all values in a dataframe \n",
    "# column by:\n",
    "# - ds[column_name].mean()\n",
    "# - ds[column_name].median()\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "bulk_dens_predicted = []\n",
    "\n",
    "for index, row in dpred_in.iterrows():\n",
    "    bulk_ec = row[ec_colname]\n",
    "    if not fixed:\n",
    "        vwc = row['vwc [%]']\n",
    "        sand = row['sand [%]']\n",
    "        clay = row['clay [%]']\n",
    "        water_ec = row['hydra_ecp [mS/m]']\n",
    "    else:\n",
    "        vwc = vwc_f\n",
    "        sand = sand_f\n",
    "        clay = clay_f\n",
    "        water_ec = water_ec_f\n",
    "    bd_pred = predict_bd(bulk_ec, vwc, sand, clay, water_ec)\n",
    "    bulk_dens_predicted.append(bd_pred)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=[12, 5])\n",
    "# Plot predicted property vs EC\n",
    "axes.scatter(ds['bd [g/cm3]'], bulk_dens_predicted, alpha=0.7, c = 'red')\n",
    "axes.set_xlabel(\"observed bulk density [g/cm3]\")\n",
    "axes.set_ylabel(\"Predicted bulk density [g/cm3]\")\n",
    "axes.set_title(f\"Predicted bulk density vs {ec_colname}\")\n",
    "#axes.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. FDEM data inversion\n",
    "\n",
    "Until now, you have worked with raw FDEM data. The QP output of the instrument is given as LIN ECa data, apparent EC data that consider a simple, linear relationship between the subsurface EC, and the QP instrument output. Hereby, the EC of all materials influencing the measurement is considered to be uniform, hence the 'apparent' nature of the data.\n",
    "  \n",
    "To evaluate the 'true' conductivity of the subsurface, we have to perform an inversion. In the following code cells, you can perform an inversion on the available FDEM datasets. More specifically, you can invert the ECa data layers collected during the survey, to model the subsurface conductivity. Through this process, you can create vertical sections (across the provided reference transect), and depth slices for specific depth ranges\n",
    "  \n",
    "To perform the inversion, we use the open-source EM modelling package [EMagPy](https://pypi.org/project/emagpy/), developed by [McLachlan et al. 2021](https://www.sciencedirect.com/science/article/pii/S0098300420305513). The sources code for this package is available on [GitHub](https://github.com/hkexgroup/emagpy/tree/master/src/emagpy).\n",
    "  \n",
    "All inversion procedures are driven by a forward model. As discussed, the forward model allows predicting the response obtained with a specific geophysical instrument used in a specific configuration, given an assumed subsurface model, described by a set of parameters. The underlying model builds on the sensitivities plotted in code block 1.2, only it integrates the cumulative sensitivity (CS) functions that describe how much of the total QP response (or, the ECa) of the instrument can be attributed to EC variations at different depths.\n",
    "  \n",
    "These CS functions assume that the sensitivity of the instrument depends only on the depth and the used coil configuration, and is independent of the subsurface EC and the instrument's operating frequency. More comprehensive models can be deployed as well in EMagPy, but these are beyond the scope of this exercise.\n",
    "\n",
    "In **_code cell 4.0_**, you will perform an example inversion along the reference transect. The EMagPy program reads from .csv-files, so the dataset path is the URL that is specified at the beginning of this notebook (stored in the `FDEM_transect` variable). \n",
    "You will implement the inversion in a very basic way. The initial parameter you will set is:\n",
    "- depths0: initial depths in meters of the bottom of each layer to be modelled. The bottom layer boundary is set to infinity (Numpy array with n depths).\n",
    "\n",
    "By loading the .csv files, EMagPy automatically recognises the data columns and coil configurations, and makes use of all ECa datasets to perform the inversion.\n",
    "\n",
    "All parameters of the inversion have been preset like this:\n",
    "> `transect.invert(forwardModel='CS', alpha=0.23, njobs=-1)` \n",
    "\n",
    "This includes the forward model to use (the cumulative sensitivity), an alpha parameter that smooths the inversion (fixed based on L-curve evaluation), and a parameter to optimize computation (njobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0: Performing an inversion along the reference transect\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Set the data path\n",
    "transect_path = FDEM_transect\n",
    "\n",
    "# Determine the starting model parameters\n",
    "\"\"\"\n",
    "starting depths* are generated here with np.arange** alternatively, you \n",
    "can manually create an array of depth boundary values (e.g., depths = [0.5,0.1],\n",
    "for a 3-layer model with layer boundaries at 0.5 m and 1.0 m. \n",
    "\n",
    "* the minimum boundary depth has to be larger than 0\n",
    "\n",
    "** np.arange creates a 1D array of evenly spaced values. In the example below\n",
    "this is a 1D array of from 2 to 10, with a value interval of 0.1\n",
    "\"\"\"\n",
    "\n",
    "depths_in = np.arange(0.10, 2, 0.10) # np\n",
    "\n",
    "\"\"\"\n",
    "Defining starting conductivities is optional, but if you remove (or comment)\n",
    "the line above, you have to remove that argument from the .setInit method \n",
    "below as `transect.setInit(depths0=depths_in)`.\n",
    "\"\"\"\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# Create an inversion problem object to solve with EMagPy\n",
    "transect = Problem()\n",
    "transect.createSurvey(transect_path, freq=9000,hx=0.165,unit='ECa')\n",
    "transect.setInit(depths0=depths_in)\n",
    "\n",
    "# Run the inversion\n",
    "transect.invert(forwardModel='CS', alpha=0.13, njobs=-1)\n",
    "print('   Finished inversion')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting and exporting results\n",
    "\n",
    "By running **_code cell 4.1_** you can plot the inversion results, and put these in a dataframe. \n",
    "Running the cell also exports the inversion results as a csv-file (you can modify the title if wanted). From the dataframe, you can then extract a single profile and plot it. In **_code cell 4.2_** this is done for a single position along the transect. You can specify the position (in meters) by changing the variable `transect_position`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1: Plot the inversion results and put outcomes into a pandas dataframe\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "csv_filename = 'inverted_transect.csv'\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# Plot inversion outcomes down to a max depth of 2 m, and plotting the data\n",
    "# based on their true coordinates along the transect (dist=True).\n",
    "transect.showResults(maxDepth= 2, dist=True, errorbar = True) \n",
    "\n",
    "# Extracting the values from the first row of the transect.depths[0] array\n",
    "depth_values = transect.depths[0][0]\n",
    "\n",
    "# Creating the custom column names for layer_cols\n",
    "layer_cols = ['EC_{:.2f}'.format(d) for d in depth_values] + ['EC_end']\n",
    "\n",
    "# Combining the data from the 'x', 'y' columns and the transect.models[0] array\n",
    "data = np.c_[transect.surveys[0].df[['x', 'y']].values, transect.models[0]]\n",
    "\n",
    "# Creating the final dataframe with the desired column names\n",
    "dt_inv = pd.DataFrame(data, columns=['x', 'y'] + layer_cols)\n",
    "dt_inv['pos'] = dt['pos']\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Export the dataframe as a csv-file\n",
    "dt_inv.to_csv('inverted_transect.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2: Plotting a single inverted profile based on its position.\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Select the row index for which you want to plot the data\n",
    "\n",
    "transect_position = 30  # profile position along the transect in meters\n",
    "                        # North = 0\n",
    "\n",
    "# ******************************************************************** #\n",
    "# Extract the data for the selected position along the transect\n",
    "closest_index = (dt_inv['pos'] - transect_position).abs().idxmin()\n",
    "row_data = dt_inv.loc[closest_index, layer_cols].values\n",
    "\n",
    "# Extract the depth values (excluding the 'EC_end' column)\n",
    "depth_values = [float(col[3:]) for col in layer_cols[:-1]]\n",
    "\n",
    "# Add the ending depth value (assuming equal spacing between depth values)\n",
    "depth_values.append(depth_values[-1] + (depth_values[-1] - depth_values[-2]))\n",
    "\n",
    "# Create the plot\n",
    "plt.figure()\n",
    "plt.step(row_data, depth_values)\n",
    "plt.xlabel('EC [mS/m]')\n",
    "plt.ylabel('Depth')\n",
    "plt.title('EC values at {} m on the transect'.format(transect_position))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling EC variations at sampled locations\n",
    "\n",
    "By running **_code cell 4.3_** you can perform an inversion on the FDEM data collected at the locations where you took invasive samples. You can set a specific depth extent for the subsurface layers. Instead of modelling the conductivity for each 10 cm layer as in the transect inversion, try setting the layer boundaries based on your prior knowledge of the soil buildup, instead of modelling the conductivity for 10 cm thick layers as in the example above.\n",
    "This prior knowledge can be derived from the samples you took on the field, most importantly the soil profile descriptions and the downhole conductivity measurements.\n",
    "\n",
    "You will instantly plot the results, and write the outcome to a pdf. If you are happy with the results, and want to have these data in a csv file run **_code cell 4.4_** to export these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3: Inversion at the sampling locations\n",
    "# -----------------------------------------\n",
    "\n",
    "# Determine the inversion parameters (layer boundary depths & conductivities)\n",
    "'''\n",
    "Here is an example for a 2 layer model with boundaries at 0.5 and 1 m. \n",
    "You can adjust de starting depths array (`depths_in`). Try creating a \n",
    "3-layer model based on the observations you made on the field\n",
    "'''\n",
    "\n",
    "depths_in = [0.5,1.0] \n",
    "#depths_in = np.arange(0.10, 1.75, 0.10) \n",
    "\n",
    "# Set pdf name for exporting plot\n",
    "pdf_name = 'Modelled_EC_profiles.pdf'\n",
    "\n",
    "# ******************************************************************** #\n",
    "# Create an inversion problem object to solve with EMagPy\n",
    "\n",
    "sampling_path = samples\n",
    "sample_transect = Problem()\n",
    "sample_transect.createSurvey(sampling_path, freq=9000,hx=0.165,unit='ECa')\n",
    "sample_transect.setInit(depths0=depths_in)\n",
    "\n",
    "# Run the inversion\n",
    "sample_transect.invert(forwardModel='CS', alpha=0.23, njobs=-1)\n",
    "print('   Finished inversion')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Plotting and exporting the EC profiles figure as a pdf\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Putting the results in a dataframe, and concatenate with analytical data.\n",
    "depth_values = sample_transect.depths[0][0]\n",
    "layer_cols = ['EC_{:.2f}'.format(d) for d in depth_values] + ['EC_end']\n",
    "data = np.c_[sample_transect.surveys[0].df[['x', 'y']].values, \n",
    "             sample_transect.models[0]]\n",
    "\n",
    "ds_inv = pd.DataFrame(data, columns=['x', 'y'] + layer_cols)\n",
    "selected_columns_ds = pd.concat([ds.iloc[:, 3:8], ds.iloc[:, 17:]], axis=1)\n",
    "ds_all = pd.concat([ds_inv, selected_columns_ds], axis=1)\n",
    "ds_all = ds_all.dropna()\n",
    "\n",
    "unique_sample_ids = ds_all['ID'].unique()\n",
    "\n",
    "# Calculate number of rows and columns for the subplot\n",
    "subplot_rows = 3\n",
    "subplot_cols = 5\n",
    "\n",
    "# Get axis limits\n",
    "global_x_min = ds_all[layer_cols].min().min()\n",
    "global_x_max = ds_all[layer_cols].max().max()\n",
    "\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(subplot_rows, subplot_cols, figsize=(15, 9))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "\n",
    "# Loop through each sample ID and plot the EC values\n",
    "for i, sample_id in enumerate(unique_sample_ids):\n",
    "    # Get row index for the current sample ID\n",
    "    row_index = ds_all.loc[ds_all['ID'] == sample_id].index[0]\n",
    "    row_data = ds_all.loc[row_index, layer_cols].values\n",
    "\n",
    "    # Extract depth values\n",
    "    depth_values = [float(col[3:]) for col in layer_cols[:-1]]\n",
    "    depth_values.append(depth_values[-1] + (depth_values[-1] - depth_values[-2]))\n",
    "\n",
    "    # Get the current subplot axis\n",
    "    ax = axes[i // subplot_cols, i % subplot_cols]\n",
    "\n",
    "    # Plot the data\n",
    "    ax.step(row_data, depth_values)\n",
    "    ax.set_xlabel('EC [mS/m]')\n",
    "    ax.set_ylabel('Depth')\n",
    "    ax.set_title(f'Sample {int(sample_id)}')\n",
    "    ax.set_xlim(global_x_min, global_x_max)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True)\n",
    "\n",
    "fig.suptitle('Modelled EC profiles at sampling locations', fontsize=14)\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "with PdfPages(pdf_name) as pdf:\n",
    "    pdf.savefig()\n",
    "\n",
    "# Show the subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4: Export as a csv-file\n",
    "csv_filename = 'samples_inverted.csv'\n",
    "ds_all.to_csv(csv_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse modelling and predicting soil properties\n",
    "\n",
    "Now that you have predicted EC values for specific soil layers, you try to predict the target soil properties based on these modelled values. You can do this by using the regression functions that you developed above, or you can apply the pedophysical model from Linde et al. 2006.\n",
    "\n",
    "**_Code cell 4.5_** is simply a copy of **_Code cell 3.6a_**, but using the dataframe that contains the inversion results at the sampling locations. The example is given for water content, but for the other properties, simply replace the code in cell **_4.5_** with the code from cell **_3.6b_** or **_3.6c_**. Just replace ds.copy() in the first code line with ds_all.copy()\n",
    "\n",
    "The EC layer column names are structured as 'EC_[depth_boundary]'. If you have specified `depths_in = [0.5,1.0]`, then the column names with the predicted EC values are: `'EC_0.50'`, `'EC_1.00'`, `'EC_end'`. The last layer is always named 'EC_end'. Make sure to enter the correct column name in the `ec_colname` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5: Predicting water content for all rows in a dataframe\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "dpred_in = ds_all.copy() # If needed, you can replace ds.copy with another dataframe\n",
    "\n",
    "# Set the dataframe column name that holds the EC values \n",
    "ec_colname = 'EC_0.50' # or any other EC(a) dataset, e.g., 'HCP1.0' \n",
    "\n",
    "# Set to true if you want to fix the given properties for all datapoints\n",
    "fixed = False\n",
    "\n",
    "sand_f = dpred_in['sand [%]'].mean() #sand content\n",
    "clay_f = dpred_in['clay [%]'].mean() #clay content\n",
    "bd_f = dpred_in['bd [g/cm3]'].mean() #bulk density\n",
    "water_ec_f = dpred_in['hydra_ecp [mS/m]'].mean() #ec_water\n",
    "\n",
    "# NOTE: you can get the mean or median value of all values in a dataframe \n",
    "# column by:\n",
    "# - ds[column_name].mean()\n",
    "# - ds[column_name].median()\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "vwc_predicted = []\n",
    "\n",
    "for index, row in dpred_in.iterrows():\n",
    "    bulk_ec = row[ec_colname]\n",
    "    if not fixed:\n",
    "        sand = row['sand [%]']\n",
    "        clay = row['clay [%]']\n",
    "        bd = row['bd [g/cm3]']\n",
    "        water_ec = row['hydra_ecp [mS/m]']\n",
    "    else: \n",
    "        sand = sand_f\n",
    "        clay = clay_f\n",
    "        bd = bd_f\n",
    "        water_ec = water_ec_f\n",
    "\n",
    "    vwc_pred = predict_water(bulk_ec, sand, clay, bd, water_ec) #(bulk_ec, sand, clay, bd, water_ec)\n",
    "    vwc_predicted.append(vwc_pred)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=[12, 5])\n",
    "# Plot predicted property vs EC\n",
    "axes.scatter(dpred_in['vwc [%]'], vwc_predicted, alpha=0.7, c = 'blue')\n",
    "axes.set_xlabel(\"observed water content [%]\")\n",
    "axes.set_ylabel(\"Predicted water content [%]\")\n",
    "axes.set_title(f\"Observed vs predicted water content with {ec_colname}\")\n",
    "for index, row in dpred_in.iterrows():\n",
    "    axes.text(row['vwc [%]'], vwc_predicted[index], str(int(row['ID'])), \n",
    "            fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Putting it all together\n",
    "\n",
    "After evaluating the outcomes of the inversion conducted in the previous cells, you can apply the inversion to the entire survey dataset. Hereby, you can target specific layers in the subsurface, and model their electrical conductivity. This follows the same flow as in the previous code cells, only now you will not visualise the data along a transect (vertical slice), but you will create horizontal EC slices that represent the electrical conductivities of the layers you define by setting the model depths.\n",
    "\n",
    "Considering the availability of sampling information from the upper part of the soil, your main goal is to predict the spatial variability of your target properties across the entire survey area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture --no-stdout\n",
    "# runtime when creating 3-layer model: ca. 10 mins on Google Colaboratory\n",
    "\n",
    "file_path = FDEM_surveydata\n",
    "survey = Problem()\n",
    "survey.createSurvey(file_path, freq=9000,hx=0.165)\n",
    "survey.setInit(depths0=[0.40, 1.0])\n",
    "survey.invert(forwardModel='CS',\n",
    "              method='L-BFGS-B',\n",
    "              alpha=0.23,njobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map conductivity per layer \n",
    "survey.showSlice(islice=0, contour=True)\n",
    "#survey.showSlice(islice=1, contour=True, vmin=0, vmax=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put inversion results in dataframe \n",
    "# ----------------------------------\n",
    "# layer_cols = ['EC {:d}'.format(a+1) for a in range(transect.models[0].shape[1])]\n",
    "# depth_cols = ['depth {:d}'.format(a+1) for a in range(transect.depths[0].shape[1])]\n",
    "# data = np.c_[transect.surveys[0].df[['x','y']].values, transect.models[0], transect.depths[0]]\n",
    "# inv_df = pd.DataFrame(data, columns=['x','y'] + layer_cols + depth_cols)\n",
    "# inv_df.head()\n",
    "\n",
    "###\n",
    "\n",
    "# layer_cols = ['EC {:d}'.format(a+1) for a in range(transect.models[0].shape[1])]\n",
    "# depth_cols = ['depth {:d}'.format(a+1) for a in range(transect.depths[0].shape[1])]\n",
    "# data = np.c_[transect.surveys[0].df[['x','y']].values, transect.models[0], transect.depths[0]]\n",
    "# inv_ds = pd.DataFrame(data, columns=['x', 'y'] + layer_cols + depth_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# islice_n = 1\n",
    "# survey.showSlice(islice=islice_n, contour=True, vmin=0, vmax=120)\n",
    "#inv_df.head()\n",
    "#df.head()\n",
    "inv_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64741f101c4c61109c751d1cf312fb9d03f24ebf08ef89d3abcfdde998172cbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
